{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction aux polynômes de chaos : estimation des coefficients\n",
    "\n",
    "TODO : estimation creuse par sélection de modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé\n",
    "\n",
    "Dans cette page, nous présentons la décomposition en polynômes du chaos d'une fonction multivariée. Nous présentons les méthodes par intégration et par moindres carrés linéaires. Puis nous présentons le problème du sur-apprentissage ainsi que les méthodes de validation croisée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "OpenTURNS :\n",
    "* http://openturns.github.io/openturns/master/theory/meta_modeling/chaos_basis.html\n",
    "* http://openturns.github.io/openturns/master/theory/meta_modeling/polynomial_least_squares.html\n",
    "* http://openturns.github.io/openturns/master/theory/meta_modeling/cross_validation.html\n",
    "* http://openturns.github.io/openturns/master/theory/meta_modeling/polynomial_sparse_least_squares.html\n",
    "\n",
    "Ouvrages, articles, thèses :\n",
    "* O.P Le Maître, O.M.Knio, Spectral methods for uncertainty quantification, Springer, 2010\n",
    "* Wuan Luo. Wiener Chaos Expansion and Numerical Solutions of Stochastic Stochastic Partial Differential Equations. PhD thesis, California Institute of Technology, May 2006.\n",
    "* R. H. Cameron and W. T. Martin. The orthogonal development of nonlinear functionals in series of Fourier-Hermite functionals. Annals of Mathematics, 48(2):385-392, April 1947.\n",
    "* Adaptive sparse polynomial chaos expansions for uncertainty propagation and sensitivity analysis, Thèse, Géraud Blatman, 2009\n",
    "\n",
    "Pour les plus aventuriers, vous pouvez prendre de risque de consulter mon propre texte qui contient la démonstration de plusieurs résultats présentés ici.\n",
    "* Introduction to polynomials chaos with NISP. Michael Baudin. Version 0.5. March 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothèses\n",
    "\n",
    "On considère une variable aléatoire standardisée $\\xi\\in\\mathbb{R}^p$ où $n$ est la dimension de l'entrée. On considère la fonction $h$  :\n",
    "$$\n",
    "Y = g(\\xi)\n",
    "$$\n",
    "où $Y\\in\\mathbb{R}$. \n",
    "\n",
    "*Note* : On considère ici le cas où la sortie $Y$ est un scalaire. Le cas plus général où la sortie $Y$ est un vecteur se traite de manière analogue, pour chaque composante du vecteur de sortie. Plus précisément, cela revient à créer plusieurs polynômes du chaos, chaque polynôme étant associé à une composante du vecteur de sortie. Cela ne nécessite pas nécessairement de nouveaux appels à la la fonction $g$, mais requiert des calculs supplémentaires. \n",
    "\n",
    "Supposons que la variance de la sortie $Y$ est finie et que les marginales de $\\xi$ sont indépendantes. Sous ces hypothèses, notons $f_i$ la loi marginale de la variable aléatoire $\\xi_i\\in\\mathbb{R}$, pour $i=1,...,n$. Puisque les variables sont indépentantes, la densité de probabilité du vecteur aléatoire $\\xi$ est le produit des lois marginales, c'est à dire :\n",
    "$$\n",
    "f(\\xi) = \\prod_{i=1}^p f_i(\\xi_i)\n",
    "$$\n",
    "pour tout $\\xi\\in\\mathbb{R}^p$. \n",
    "\n",
    "On considère la décomposition en chaos polynomiale :\n",
    "$$\n",
    "\\tilde{g}(\\xi) = \\sum_{j=0}^P a_j\\psi_j(\\xi)\n",
    "$$\n",
    "pour tout $\\xi\\in\\mathbb{R}^p$. \n",
    "\n",
    "L'objectif de cette page est de présenter les méthodes d'estimation des coefficients $a=(a_0,a_1,....,a_P)^T\\in\\mathbb{R}^{P+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthodes spectrales non intrusives\n",
    "\n",
    "On appelle méthodes spectrales non intrusives (NISP) les méthodes qui considèrent la fonction comme une boîte noire. L'objectif de ces méthodes est de calculer le vecteur des coeffients $a\\in\\mathbb{R}^{P+1}$ de la décomposition en chaos polynomial. \n",
    "\n",
    "Il y a deux grandes familles de méthodes pour calculer les coefficients de la décomposition en polynômes de chaos.\n",
    "* Les méthodes par intégration consistent à évaluer l'intégrale associée au produit scalaire définissant le coefficient $a_j$. \n",
    "* Les méthodes par moindres carrés linéaires consistent à calculer le vecteur des coefficients en résolvant un problème de moindres carrés linéaires dans lequel la base de fonctions est la base polynomiale multivariées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthodes par intégration\n",
    "\n",
    "Dans les méthodes par intégration, on évalue l'intégrale :\n",
    "$$\n",
    "a_j=(g,\\psi_j) = \\int_{\\mathbb{R}^p} g(\\xi)\\psi_j(\\xi)f(\\xi)d\\xi,\n",
    "$$\n",
    "pour $j=0,1,...,P$.\n",
    "\n",
    "Plusieurs méthodes sont envisageables pour estimer la valeur de l'intégrale.\n",
    "* La méthode la plus simple repose sur l'intégration par la méthode de Monte-Carlo simple, mais toute méthode d'échantillonage pour estimer une intégrale est élligible, comme l'utilisation d'un plan LHS ou une séquence à faible discrépance par exemple. \n",
    "* On peut également utiliser les méthodes de quadrature de Gauss multidimensionnelles par tensorisation. Toutefois, les méthodes de tensorisation de quadrature de Gauss sont associés à un grand nombre de noeuds, ce qui mène à un grand nombre d'évaluations de la fonction $g$. \n",
    "* Pour limiter la difficulté, les méthodes de quadratures imbriquées et de grilles creuses (*sparse grids* en anglais) permettent de réduire le nombre de noeuds d'intégration (voir (Le Maître, Knio, 2010) pages 51-63). \n",
    "\n",
    "L'inconvénient des méthodes déterministes (comme la quadrature de Gauss par exemple) pour l'estimation des intégrales requises est que le nombre d'évaluations de la fonction $g$ est fixé à l'avance par l'algorithme. Ce n'est pas très pratique, surtout dans les situations où on souhaite ajouter des points dans le plan d'expériences pour améliorer la qualité du métamodèle. Dans un tel contexte, il peut être intéressant d'envisager les méthodes d'estimations par moindres carrés, présentées dans la prochaine section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthodes par moindres carrés\n",
    "\n",
    "Dans les méthodes par moindres carrés linéaires les plus simples, on considère un plan d'expériences de type Monte-Carlo simple. Soit $n\\in\\mathbb{N}$ est un entier représentant la taille de l'échantillon. Soit $\\left\\{x^{(j)}\\in\\mathbb{R}^p\\right\\}_{j=1,...,n}$ un échantillon i.i.d. du vecteur aléatoire $X$. \n",
    "Par conséquent, la réalisation $x_i^{(j)}$ est la i-ème composante de la j-ème réalisation, pour $i=1,...,p$ et $j=1,...,n$. \n",
    "\n",
    "Soit $y\\in\\mathbb{R}^n$ le vecteur des observations de la sortie :\n",
    "$$\n",
    "y^{(j)} = g\\left(x^{(j)}\\right),\n",
    "$$\n",
    "pour $j=1,...,n$.\n",
    "\n",
    "Pour calculer les variables standardisées $\\left\\{\\xi^{(j)}\\right\\}_{j=1,...,n}$, on considère l'équation \n",
    "$$\n",
    "\\xi_i^{(j)} = T_i\\left(X_i^{(j)}\\right),\n",
    "$$\n",
    "pour $i=1,\\dots,p$ et $j=1,...,n$.\n",
    "\n",
    "La matrice de conception $Z\\in\\mathbb{R}^{n\\times (P+1)}$ définie par :\n",
    "$$\n",
    "z_{ji} = \\psi_i\\left(\\xi^{(j)}\\right)\n",
    "$$\n",
    "pour $i=0,\\dots,P+1$ et $j=1,...,n$. \n",
    "Alors le vecteur des prédictions du chaos polynomial est $Za$ où $a\\in\\mathbb{R}^{P+1}$ est \n",
    "le vecteur des coeffients  de la décomposition en chaos polynomial. \n",
    "La méthode des moindres carrés linéaires consiste à minimiser la norme euclidienne des écarts entre les observations du modèle et les prédictions du chaos polynomial:\n",
    "$$\n",
    "\\hat{a} = \\textrm{argmin}_{a\\in\\mathbb{R}^{P+1}} \\|y - Za\\|_2,\n",
    "$$\n",
    "où $\\|\\cdot\\|_2$ est la norme euclidienne en dimension $n$. \n",
    "Si la matrice $Z$ est de rang plein, alors la solution est donnée par les équations normales \n",
    "$$\n",
    "\\hat{a} = \\left(Z^T Z\\right)^{-1} Z^T y.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surapprentissage et chaos polynomial\n",
    "\n",
    "Sur la base de la fonction $g$, l'étape d'apprentissage a produit un métamodèle $\\tilde{g}$. \n",
    "Si nous connaissions la fonction $g$, nous pourrions calculer l'erreur quadratique moyenne $Err$ définie par :\n",
    "$$\n",
    "Err = E\\left[\\left(g(X)-\\tilde{g}(X)\\right)^2\\right].\n",
    "$$\n",
    "\n",
    "Malheureusement, estimer $Err$ peut nécessiter un grand nombre d'évaluations de $g$, peut être plus que pour estimer les coefficients du métamodèle $\\tilde{g}$.\n",
    "Pour contourner ce problème, considérons le coefficient $R^2$ pour estimer la qualité du métamodèle sur la base d'apprentissage (d'où l'indice \"a\"). \n",
    "Supposons que le plan d'expériences $\\left\\{x^{(j)}_a\\right\\}_{j=1,...,n}$ est utilisé pour estimer les coefficients d'un modèle de regression linéaire tel qu'un métamodèle de chaos polynomial par exemple. \n",
    "$$\n",
    "R^2(g(x_a),\\tilde{g}(x_a)) = 1 - \\frac{ \\sum_{j=1}^N \\left( y^{(j)}_a - \\tilde{y}^{(j)}_a \\right)^2  }{ \\sum_{j=1}^N \\left( y^{(j)}_a - \\bar{y}_a \\right)^2 }\n",
    "$$\n",
    "où $\\bar{y}_a = \\frac{1} {N} \\sum_{i=1}^N y^{(j)}_a$.\n",
    "\n",
    "La difficulté est la suivante: quand le nombre de coefficients dans la décomposition augmente, le $R^2$ diminue. La difficulté essentielle est que, même si le $R^2$ est petit, le métamodèle peut ne pas être de qualité suffisante (voir (Blatman, 2009) page 83). Le problème de la généralisation du métamodèle consiste à obtenir un métamodèle de qualité.  \n",
    "\n",
    "Supposons qu'un second plan d'expériences de validation (d'où l'indice \"v\") du métamodèle est généré : $\\left\\{x^{(j)}_v\\right\\}_{j=1,...,n}$. L'objectif de ce second plan d'expériences est de tester le métamodèle sur des points d'entrées que le métamodèle n'a pas considéré pour estimer ses coefficients.\n",
    "\n",
    "Soient $g(x_v)$ et $\\tilde{g}(x_v)$ les sorties du modèle et du métamodèle sur le plan d'expériences de validation. \n",
    "Dans ce contexte, le coefficient $Q^2$ est égal au coefficient $R^2$ appliqué au plan d'expériences de validation :\n",
    "$$\n",
    "Q^2 = R^2(g(x_v),\\tilde{g}(x_v)).\n",
    "$$\n",
    "\n",
    "Il s'avère que, en général, lorsque le nombre de coefficients dans le métamodèles augmente, le $R^2$ diminue tout le temps, tandis que le $Q^2$ diminue d'abord, puis augmente. \n",
    "\n",
    "D'un côté, le coefficient $R^2$ ne permet pas de mesurer efficacement la prédictivité du métamodèle. D'un autre côté, le coefficient $Q^2$ permet de bien quantifier la prédictivité, mais nécessite des nouvelles évaluations de la fonction $g$ sur un plan d'expériences de validation. Une méthode plus robuste d'estimation de l'erreur de validation que le $R^2$ et moins coûteuse que le $Q^2$ est la méthode *leave-one-out*, présentée dans la suite.\n",
    "\n",
    "Le problème du surapprentissage est lié au nombre d'observations et au nombre de coefficients : pour obtenir un métamodèle de qualité, il est préférable d'avoir un nombre relativement faible de coefficients par rapport au nombre d'observations. La difficulté est que, pour obtenir un chaos polynomial dont la prédictivité est élevée, il est parfois nécessaire d'augmenter le degré du polynôme. En conséquence, le nombre de coefficients augmente, ce qui provoque un surapprentissage. Un premier moyen pour limiter ce problème consiste à utiliser une règle d'énumération creuse qui favorise des termes élevés sans trop augmenter le nombre de coefficients liés aux interactions entre les variables. Un second moyen pour limiter le problème consiste à utiliser une méthode de sélection de modèle, ce qui est présenté dans la section suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold et leave-one-out\n",
    "\n",
    "La méthode de validation croisée consiste à découper l'échantillon en deux, le premier étant utilisé pour l'estimation des coefficients et le second pour la validation. Cette méthode a l'inconvénient de laisser de côté une grande partie de l'échantillon, qui n'est pas utilisée pour l'apprentissage. Pour limiter le problème, un entier $K$ étant donné, la méthode du K-Fold est un raffinement qui consiste à découper l'échantillon en K sous-échantillons de tailles presques égales. L'échantillon d'apprentissage est alors constitué de tous les sous-échantillons sauf un, qui est utilisé pour la validation. La méthode leave-one-out (LOO) peut alors être vue comme un cas particulier de K-Fold avec K=1. \n",
    "\n",
    "Dans la méthode classique, on considère un plan d'expériences \n",
    "$$\n",
    "\\mathcal{X} = \\left\\{x^{(j)}\\in\\mathbb{R}^p\\right\\}_{j=1,...,n}\n",
    "$$\n",
    "de taille $n$ et on estime les coefficients du métamodèle $\\tilde{g}$. Dans la méthode leave-one-out, pour $j=1,...,n$, on considère le plan d'expériences $\\mathcal{X}^{(-j)}$ dans lequel la j-ème expérience a été retirée :\n",
    "$$\n",
    "\\mathcal{X}^{(-j)} = \\left\\{x^{(1)},...,x^{(j-1)},x^{(j+1)},...,,x^{(n)}\\in\\mathbb{R}^p\\right\\}_{j=1,...,n}.\n",
    "$$\n",
    "Notons $\\tilde{g}^{(-j)}$ le métamodèle dont les coefficients ont étés appris sur le plan d'expériences $\\mathcal{X}^{(-j)}$. \n",
    "L'erreur de prédiction au point $x^{(j)}$ est alors la différence entre la prédiction du modèle $g$ et la prédiction du métamodèle LOO $\\tilde{g}^{(-j)}$ :\n",
    "$$\n",
    "\\Delta^{(j)} = g\\left(x^{(j)}\\right) - \\tilde{g}^{(-j)}\\left(x^{(j)}\\right)\n",
    "$$\n",
    "pour $j=1,...,n$. En d'autres termes, on évalue le résidu pour un point $x^{(j)}$ qui n'a pas été observé par le métamodèle LOO $\\tilde{g}^{(-j)}$. \n",
    "\n",
    "L'erreur LOO est alors la moyenne empirique des erreurs :\n",
    "$$\n",
    "E_{LOO} = \\frac{1}{n} \\sum_{j=1}^n \\left(\\Delta^{(j)}\\right)^2.\n",
    "$$\n",
    "\n",
    "Le coefficient de prédictivité $Q^2$ est alors calculé par :\n",
    "$$\n",
    "Q^2 = 1 - \\frac{E_{LOO}}{\\hat{V}(Y)}\n",
    "$$\n",
    "où $\\hat{V}(Y)$ est la variance du vecteur des sorties. \n",
    "En pratique, cette variance est estimée sur la base de l'échantillon. \n",
    "Remarquons que, lorsque l'erreur LOO $E_{LOO}$ augmente, alors le $Q^2$ diminue.\n",
    "\n",
    "La difficulté de l'expression de l'erreur de prédiction LOO est qu'elle nécessite de calculer les coefficients du métamodèle leave-one-out $\\tilde{g}^{(-j)}$, ce qui peut être coûteux. \n",
    "Soit $H$ la matrice chapeau des moindres carrés linéaires :\n",
    "$$\n",
    "H = Z (Z^T Z) Z^T.\n",
    "$$\n",
    "Pour un modèle de regression linéaire comme le chaos polynomial par moindres carrés linéaires, la matrice de projection $H$ est la matrice qui projette les observations $y$ en les prédictions $\\hat{y} = H y$. On peut démontrer que l'erreur LOO est égale à :\n",
    "$$\n",
    "\\Delta^{(j)} = \\frac{g\\left(x^{(j)}\\right) - \\tilde{g}\\left(x^{(j)}\\right)}{1-h_{jj}}\n",
    "$$\n",
    "où $h_{jj}$ est le j-ème terme diagonal de la matrice chapeau $H$. \n",
    "\n",
    "En pratique, l'expression précédente est estimée en substituant le métamodèle $\\tilde{g}$ à la place du modèle $g$. \n",
    "\n",
    "Le coefficient $Q^2$ précédent peut être corrigé pour tenir compte du nombre de paramètres estimés dans le modèle. L'objectif de cette erreur LOO corrigée est d'augmenter l'erreur lorsque le nombre de paramètres augmente. On considère alors l'erreur *ajustée* définie par :\n",
    "$$\n",
    "E_{LOO}^\\star = E_{LOO} \\frac{n-1}{n-P-1}.\n",
    "$$\n",
    "L'erreur LOO *corrigée* est une variante permettant de prendre en compte les situations où la taille de l'échantillon est petite. \n",
    "\n",
    "Plus de détails sur ce thème sont donnés dans (Blatman, 2009) pages 85 et 86."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthodes de sélection de modèle\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
