{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deterministic code calibration\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "$\\newcommand{\\Rset}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Nset}{\\mathbb{R}}$\n",
    "$\\newcommand{\\vect}[1]{\\boldsymbol{#1}}$\n",
    "\n",
    "## References\n",
    "\n",
    "   -  N. H. Bingham and John M. Fry (2010). *Regression, Linear Models in Statistics*, Springer Undergraduate Mathematics Series. Springer.\n",
    "\n",
    "   - S. Huet, A. Bouvier, M.A. Poursat, and E. Jolivet (2004). *Statistical Tools for Nonlinear Regression*, Springer.\n",
    "\n",
    "   - C. E. Rasmussen and C. K. I. Williams (2006), *Gaussian Processes for Machine Learning*, The MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We consider a computer model $\\vect{h}$ (i.e. a deterministic\n",
    "function) to calibrate:\n",
    "\n",
    "$$\n",
    "       \\vect{z} = \\vect{h}(\\vect{x}, \\vect{\\theta}),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "-  $\\vect{x} \\in \\Rset^{d_x}$ is the input vector;\n",
    "-  $\\vect{z} \\in \\Rset^{d_z}$ is the output vector;\n",
    "-  $\\vect{\\theta} \\in \\Rset^{d_h}$ are the unknown parameters of\n",
    "   $\\vect{h}$ to calibrate.\n",
    "\n",
    "Let $n \\in \\Nset$ be the number of observations. The standard\n",
    "hypothesis of the probabilistic calibration is:\n",
    "\n",
    "$$\n",
    "       \\vect{Y}^i = \\vect{z}^i + \\vect{\\varepsilon}^i,\n",
    "$$\n",
    "\n",
    "for $i=1,...,n$ where $\\vect{\\varepsilon}^i$ is a random\n",
    "measurement error such that:\n",
    "\n",
    "$$\n",
    "       E(\\varepsilon)=\\vect{0} \\in \\Rset^{d_z}, \\qquad Cov(\\varepsilon)=\\Sigma \\in \\Rset^{d_z\\times d_z},\n",
    "$$\n",
    "\n",
    "where $\\Sigma$ is the error covariance matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goal of calibration is to estimate $\\vect{\\theta}$, based on\n",
    "observations of $n$ inputs\n",
    "$(\\vect{x}^1, \\ldots, \\vect{x}^n)$ and the associated $n$\n",
    "observations of the output $(\\vect{y}^1, \\ldots, \\vect{y}^n)$. In\n",
    "other words, the calibration process reduces the discrepancy between the\n",
    "observations $(\\vect{y}^1, \\ldots, \\vect{y}^n)$ and the\n",
    "predictions $\\vect{h}(\\vect{\\theta})$. Given that\n",
    "$(\\vect{y}^1, \\ldots, \\vect{y}^n)$ are realizations of a random\n",
    "variable, the estimate of $\\vect{\\theta}$, denoted by\n",
    "$\\hat{\\vect{\\theta}}$, is also a random variable. Hence, the\n",
    "secondary goal of calibration is to estimate the distribution of\n",
    "$\\hat{\\vect{\\theta}}$ representing the uncertainty of the\n",
    "calibration process.\n",
    "\n",
    "The standard observation model makes the hypothesis that the covariance\n",
    "matrix of the error is diagonal, i.e.\n",
    "\n",
    "$$\n",
    "       \\Sigma = \\sigma^2 {\\bf I}\n",
    "$$\n",
    "\n",
    "where $\\sigma^2 \\in \\Rset$ is the constant observation error\n",
    "variance.\n",
    "\n",
    "In the remaining of this section, the input $\\vect{x}$ is not\n",
    "involved anymore in the equations. This is why we simplify the equation\n",
    "into:\n",
    "\n",
    "$$\n",
    "       \\vect{z} = \\vect{h}(\\vect{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least squares\n",
    "\n",
    "The residuals is the difference between the observations and the\n",
    "predictions:\n",
    "\n",
    "$$\n",
    "       \\vect{r}^i = \\vect{y}^i - \\vect{h}(\\vect{\\theta})^i\n",
    "$$\n",
    "\n",
    "for $i=1,...,n$. The method of least squares minimizes the square\n",
    "of the euclidian norm of the residuals. This is why the least squares\n",
    "method is based on the cost function $C$ defined by:\n",
    "\n",
    "$$\n",
    "       C(\\vect{\\theta}) = \\frac{1}{2} \\|\\vect{y} - \\vect{h}(\\vect{\\theta})\\|^2 = \\frac{1}{2} \\sum_{i=1}^n \\left( \\vect{y}^i - \\vect{h}(\\vect{\\theta})^i \\right)^2,\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta} \\in \\Rset^{d_h}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The least squares method minimizes the cost function $C$:\n",
    "\n",
    "$$\n",
    "       \\hat{\\vect{\\theta}} = \\argmin_{\\vect{\\theta} \\in \\Rset^{d_h}} \\frac{1}{2} \\|\\vect{y} - \\vect{h}(\\vect{\\theta})\\|^2.\n",
    "$$\n",
    "\n",
    "The unbiased estimator of the variance is:\n",
    "\n",
    "$$\n",
    "       \\hat{\\sigma}^2 = \\frac{\\|\\vect{y} - \\vect{h}(\\vect{\\theta})\\|^2}{n - d_h}.\n",
    "$$\n",
    "\n",
    "Notice that the previous estimator is not the maximum likelihood\n",
    "estimator (which is biased)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear least squares\n",
    "\n",
    "In the particular case where the deterministic function $\\vect{h}$\n",
    "is linear with respect to the parameter $\\vect{\\theta}$, then the\n",
    "method reduces to the linear least squares. Let\n",
    "$J \\in \\Rset^{n \\times d_h}$ be the Jacobian matrix made of the\n",
    "partial derivatives of $\\vect{h}$ with respect to\n",
    "$\\vect{\\theta}$:\n",
    "\n",
    "$$\n",
    "       J(\\vect{\\theta}) = \\frac{\\partial \\vect{h}}{\\partial \\vect{\\theta}}.\n",
    "$$\n",
    "\n",
    "Let $\\vect{\\mu} \\in \\Rset^{d_h}$ be a reference value of the\n",
    "parameter $\\vect{\\theta}$. Let us denote by\n",
    "$J=J(\\vect{\\mu})$ the value of the Jacobian at the reference point\n",
    "$\\vect{\\mu}$. Since the function is, by hypothesis, linear, the\n",
    "Jacobian is independent of the point where it is evaluated. Since\n",
    "$\\vect{h}$ is linear, it is equal to its Taylor expansion:\n",
    "\n",
    "$$\n",
    "       \\vect{h}(\\vect{\\theta}) = \\vect{h}(\\vect{\\mu}) + J (\\vect{\\theta} - \\vect{\\mu}),\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta} \\in \\Rset^{d_h}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The corresponding linear least squares problem is:\n",
    "\n",
    "$$\n",
    "       \\hat{\\vect{\\theta}} = \\argmin_{\\vect{\\theta} \\in \\Rset^{d_h}} \\frac{1}{2} \\|\\vect{y} - \\vect{h}(\\vect{\\mu}) - J (\\vect{\\theta} - \\vect{\\mu})\\|^2.\n",
    "$$\n",
    "\n",
    "The Gauss-Markov theorem applied to this problem states that the\n",
    "solution is:\n",
    "\n",
    "$$\n",
    "       \\hat{\\vect{\\theta}} = \\vect{\\mu} + \\left(J^T J\\right)^{-1} J^T ( \\vect{y} - \\vect{h}(\\vect{\\mu})).\n",
    "$$\n",
    "\n",
    "The previous equations are the *normal equations*. Notice, however, that\n",
    "the previous linear system of equations is not implemented as is, i.e.\n",
    "we generally do not compute and invert the Gram matrix $J^T J$.\n",
    "Alternatively, various orthogonalization methods such as the QR or the\n",
    "SVD decomposition can be used to solve the linear least squares problem\n",
    "so that potential ill-conditionning of the normal equations is\n",
    "mitigated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This estimator can be proved to be the best linear unbiased estimator,\n",
    "the *BLUE*, that is, among the unbiased linear estimators, it is the one\n",
    "which minimizes the variance of the estimator.\n",
    "\n",
    "Assume that the random observations are gaussian:\n",
    "\n",
    "$$\n",
    "       \\varepsilon \\sim \\mathcal{N}(\\vect{0},\\sigma^2 {\\bf I}).\n",
    "$$\n",
    "\n",
    "Therefore, the distribution of $\\hat{\\vect{\\theta}}$ is:\n",
    "\n",
    "$$\n",
    "       \\hat{\\vect{\\theta}} \\sim \\mathcal{N}(\\vect{\\theta},\\sigma^2 J^T J).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Non Linear Least squares\n",
    "========================\n",
    "\n",
    "In the general case where the function $\\vect{h}$ is non linear\n",
    "with respect to the parameter $\\vect{\\theta}$, then the resolution\n",
    "involves a non linear least squares optimization algorithm. Instead of\n",
    "directly minimizing the squared euclidian norm of the residuals, most\n",
    "implementations rely on the residual vector, which lead to an improved\n",
    "accuracy.\n",
    "\n",
    "The difficulty in the nonlinear least squares is that, compared to the\n",
    "linear situation, the theory does not provide the distribution of\n",
    "$\\hat{\\vect{\\theta}}$ anymore.\n",
    "\n",
    "There are two practical solutions to overcome this limitation.\n",
    "\n",
    "-  bootstrap,\n",
    "-  linearization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The bootstrap method is based on the following experiment. Provided that\n",
    "we can generate a set of input and output observations, we can compute\n",
    "the corresponding value of the parameter $\\hat{\\vect{\\theta}}$.\n",
    "Reproducing this sampling experiment a large number of times would allow\n",
    "to get the distribution of the estimated parameter\n",
    "$\\hat{\\vect{\\theta}}$. In practice, we only have one single sample\n",
    "of $n$ observations. If this sample is large enough and correctly\n",
    "represents the variability of the observations, the bootstrap method\n",
    "allows to generate observations resamples, which, in turn, allow to get\n",
    "a sample of $\\hat{\\vect{\\theta}}$. An approximate distribution of\n",
    "$\\hat{\\vect{\\theta}}$ can then be computed based on kernel\n",
    "smoothing, for example. In order to get a relatively accurate\n",
    "distribution of $\\hat{\\vect{\\theta}}$, the boostrap sample size\n",
    "must be large enough. Hence, this method requires to solve a number of\n",
    "optimization problems, which can be time consuming.\n",
    "\n",
    "Alternatively, we can linearize the function $\\vect{h}$ in the\n",
    "neighbourhood of the solution $\\hat{\\vect{\\theta}}$ and use the\n",
    "gaussian distribution associated with the linear least squares. This\n",
    "method is efficient, but only accurate when the function\n",
    "$\\vect{h}$ is approximately linear with respect to\n",
    "$\\vect{\\theta}$ in the neighbourhood of\n",
    "$\\hat{\\vect{\\theta}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least squares and minimization of likelihood\n",
    "\n",
    "A link between the method of least squares and the method of maximum\n",
    "likelihood can be done provided that two hypotheses are satisfied. The\n",
    "first hypothesis is that the random output observations\n",
    "$\\vect{y}^i$ are independent. The second hypothesis is that the\n",
    "random measurement error $\\vect{\\varepsilon}$ has the gaussian\n",
    "distribution. In this particular case, it can be shown that the solution\n",
    "of the least squares problem maximizes the likelihood.\n",
    "\n",
    "This is the reason why, after a least squares calibration has been\n",
    "performed, the distribution of the residuals may be interesting to\n",
    "analyze. Indeed, if the distribution of the residuals is gaussian and if\n",
    "the outputs are independent, then the least squares estimator is the\n",
    "maximum likelihood estimator, which gives a richer interpretation to the\n",
    "solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization and ill-conditionned problems\n",
    "\n",
    "If a problem is ill-conditionned, a small change in the observations can\n",
    "generate a large change in the estimate $\\hat{\\vect{\\theta}}$.\n",
    "Hence, for problems which are ill-conditionned, calibration methods may\n",
    "include some regularization features.\n",
    "\n",
    "An ill-conditionned problem may appear in the particular case where the\n",
    "Jacobian matrix $J$ is rank-degenerate. For example, suppose that\n",
    "a linear least squares problem is considered, where some linear\n",
    "combinations of the columns of $J$ are linearily dependent. This\n",
    "implies that there is a linear subspace of the parameter space\n",
    "$\\hat{\\vect{\\theta}}$ such that linear combinations of the\n",
    "parameters do not have any impact on the output. In this case, it is not\n",
    "possible to estimate the projection of the solution on that particular\n",
    "subpace. Gaussian calibration is a way of mitigating this situation, by\n",
    "constraining the solution to be *not too far away* from a reference\n",
    "solution, named the *prior*."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
