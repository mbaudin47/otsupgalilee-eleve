{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian calibration\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "$\\newcommand{\\Rset}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Nset}{\\mathbb{R}}$\n",
    "$\\newcommand{\\vect}[1]{\\boldsymbol{#1}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We consider a computer model $\\vect{h}$ (i.e. a deterministic\n",
    "function) to calibrate:\n",
    "\n",
    "$$\n",
    "\\vect{z} = \\vect{h}(\\vect{x}, \\vect{\\theta}),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "-  $\\vect{x} \\in \\Rset^{d_x}$ is the input vector;\n",
    "-  $\\vect{z} \\in \\Rset^{d_z}$ is the output vector;\n",
    "-  $\\vect{\\theta} \\in \\Rset^{d_h}$ are the unknown parameters of\n",
    "   $\\vect{h}$ to calibrate.\n",
    "\n",
    "Let $n \\in \\Nset$ be the number of observations. The standard\n",
    "hypothesis of the probabilistic calibration is:\n",
    "\n",
    "$$\n",
    "\\vect{Y}^i = \\vect{z}^i + \\vect{\\varepsilon}^i,\n",
    "$$\n",
    "\n",
    "for $i=1,...,n$ where $\\vect{\\varepsilon}^i$ is a random\n",
    "measurement error.\n",
    "\n",
    "The goal of Gaussian calibration is to estimate $\\vect{\\theta}$,\n",
    "based on observations of $n$ inputs\n",
    "$(\\vect{x}^1, \\ldots, \\vect{x}^n)$ and the associated $n$\n",
    "observations of the output $(\\vect{y}^1, \\ldots, \\vect{y}^n)$. In\n",
    "other words, the calibration process reduces the discrepancy between the\n",
    "observations $(\\vect{y}^1, \\ldots, \\vect{y}^n)$ and the\n",
    "predictions $\\vect{h}(\\vect{\\theta})$. Given that\n",
    "$(\\vect{y}^1, \\ldots, \\vect{y}^n)$ are realizations of a random\n",
    "variable, the estimate of $\\vect{\\theta}$, denoted by\n",
    "$\\hat{\\vect{\\theta}}$, is also a random variable. Hence, the\n",
    "secondary goal of calibration is to estimate the distribution of\n",
    "$\\hat{\\vect{\\theta}}$ representing the uncertainty of the\n",
    "calibration process.\n",
    "\n",
    "In the remaining of this section, the input $\\vect{x}$ is not\n",
    "involved anymore in the equations. This is why we simplify the equation\n",
    "into:\n",
    "\n",
    "$$\n",
    "\\vect{z} = \\vect{h}(\\vect{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian calibration\n",
    "\n",
    "The bayesian calibration framework is based on two hypotheses.\n",
    "\n",
    "The first hypothesis is that the parameter $\\vect{\\theta}$ has a\n",
    "known distribution, called the *prior* distribution, and denoted by\n",
    "$p(\\vect{\\theta})$.\n",
    "\n",
    "The second hypothesis is that the output observations\n",
    "$(\\vect{y}^1, \\ldots, \\vect{y}^n)$ are sampled from a known\n",
    "conditional distribution denoted by $p(\\vect{y} | \\vect{\\theta})$.\n",
    "\n",
    "For any $\\vect{y}\\in\\Rset^{d_z}$ such that $p(\\vect{y})>0$,\n",
    "the Bayes theorem implies that the conditional distribution of\n",
    "$\\vect{\\theta}$ given $\\vect{y}$ is:\n",
    "\n",
    "$$\n",
    "p(\\vect{\\theta} | \\vect{y}) = \\frac{p(\\vect{y} | \\vect{\\theta}) p(\\vect{\\theta})}{p(\\vect{y})}\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta}\\in\\Rset^{d_h}$.\n",
    "\n",
    "The denominator of the previous Bayes fraction is independent of\n",
    "$\\vect{\\theta}$, so that the posterior distribution is\n",
    "proportional to the numerator:\n",
    "\n",
    "$$\n",
    "p(\\vect{\\theta} | \\vect{y}) \\propto  p(\\vect{y} | \\vect{\\theta}) p(\\vect{\\theta}).\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta}\\in\\Rset^{d_h}$.\n",
    "\n",
    "In the Gaussian calibration, the two previous distributions are assumed\n",
    "to be Gaussian.\n",
    "\n",
    "More precisely, we make the hypothesis that the parameter\n",
    "$\\vect{\\theta}$ has the Gaussian distribution:\n",
    "\n",
    "$$\n",
    "\\vect{\\theta} \\sim \\mathcal{N}(\\vect{\\mu}, B),\n",
    "$$\n",
    "\n",
    "where $\\vect{\\mu}\\in\\Rset^{d_h}$ is the mean of the Gaussian prior\n",
    "distribution, which is named the *background* and\n",
    "$B\\in\\Rset^{d_h \\times d_h}$ is the covariance matrix of the\n",
    "parameter.\n",
    "\n",
    "Secondly, we make the hypothesis that the output observations have the\n",
    "conditional gaussian distribution:\n",
    "\n",
    "$$\n",
    "\\vect{y} | \\vect{\\theta} \\sim \\mathcal{N}(\\vect{h}(\\vect{\\theta}), R),\n",
    "$$\n",
    "\n",
    "where $R\\in\\Rset^{d_z \\times d_z}$ is the covariance matrix of the\n",
    "output observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior distribution\n",
    "\n",
    "Denote by $\\|\\cdot\\|_B$ the Mahalanobis distance associated with\n",
    "the matrix $B$ :\n",
    "\n",
    "$$\n",
    "\\|\\vect{\\theta}-\\vect{\\mu} \\|^2_B = (\\vect{\\theta}-\\vect{\\mu} )^T B^{-1} (\\vect{\\theta}-\\vect{\\mu} ),\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta},\\vect{\\mu} \\in \\Rset^{d_h}$. Denote by\n",
    "$\\|\\cdot\\|_R$ the Mahalanobis distance associated with the matrix\n",
    "$R$ :\n",
    "\n",
    "$$\n",
    "\\|\\vect{y}-H(\\vect{\\theta})\\|^2_R = (\\vect{y}-H(\\vect{\\theta}))^T R^{-1} (\\vect{y}-H(\\vect{\\theta})).\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta} \\in \\Rset^{d_h}$ and any\n",
    "$\\vect{y} \\in \\Rset^{d_z}$. Therefore, the posterior distribution\n",
    "of $\\vect{\\theta}$ given the observations $\\vect{y}$ is :\n",
    "\n",
    "$$\n",
    "   p(\\vect{\\theta}|\\vect{y}) \\propto \\exp\\left( -\\frac{1}{2} \\left( \\|\\vect{y}-H(\\vect{\\theta})\\|^2_R \n",
    "   + \\|\\vect{\\theta}-\\vect{\\mu} \\|^2_B \\right) \\right)\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta}\\in\\Rset^{d_h}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP estimator\n",
    "\n",
    "The maximum of the posterior distribution of $\\vect{\\theta}$ given\n",
    "the observations $\\vect{y}$ is reached at :\n",
    "\n",
    "$$\n",
    "   \\hat{\\vect{\\theta}} = \\argmin_{\\vect{\\theta}\\in\\Rset^{d_h}} \\frac{1}{2} \\left( \\|\\vect{y} - H(\\vect{\\theta})\\|^2_R \n",
    "   + \\|\\vect{\\theta}-\\vect{\\mu} \\|^2_B \\right).\n",
    "$$\n",
    "\n",
    "It is called the *maximum a posteriori posterior* estimator or *MAP*\n",
    "estimator.\n",
    "\n",
    "## Regularity of solutions of the Gaussian Calibration\n",
    "\n",
    "The gaussian calibration is a tradeoff, so that the second expression\n",
    "acts as a *spring* which pulls the parameter $\\vect{\\theta}$\n",
    "closer to the background $\\vect{\\mu}$ (depending on the \"spring\n",
    "constant\" $B$, meanwhile getting as close a possible to the\n",
    "observations. Depending on the matrix $B$, the computation may\n",
    "have better regularity properties than the plain non linear least\n",
    "squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Linear Gaussian Calibration : 3DVAR\n",
    "\n",
    "The cost function of the gaussian nonlinear calibration problem is :\n",
    "\n",
    "$$\n",
    "   C(\\vect{\\theta}) = \\frac{1}{2}\\|\\vect{y}-H(\\vect{\\theta})\\|^2_R \n",
    "   + \\frac{1}{2}\\|\\vect{\\theta}-\\vect{\\mu} \\|^2_B\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta}\\in\\Rset^{d_h}$.\n",
    "\n",
    "The goal of the non linear gaussian calibration is to find the value of\n",
    "$\\vect{\\theta}$ which minimizes the cost function $C$. In\n",
    "general, this involves using a nonlinear unconstrained optimization\n",
    "solver.\n",
    "\n",
    "Let $J \\in \\Rset^{n \\times d_h}$ be the Jacobian matrix made of\n",
    "the partial derivatives of $\\vect{h}$ with respect to\n",
    "$\\vect{\\theta}$:\n",
    "\n",
    "$$\n",
    "J(\\vect{\\theta}) = \\frac{\\partial \\vect{h}}{\\partial \\vect{\\theta}}.\n",
    "$$\n",
    "\n",
    "The Jacobian matrix of the cost function $C$ can be expressed\n",
    "depending on the matrices $R$ and $B$ and the Jacobian\n",
    "matrix of the function $h$:\n",
    "\n",
    "$$\n",
    "   \\frac{d }{d\\vect{\\theta}} C(\\vect{\\theta}) \n",
    "   = B^{-1} (\\vect{\\theta}-\\vect{\\mu}) + J(\\vect{\\theta})^T R^{-1} (H(\\vect{\\theta}) - \\vect{y})\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta}\\in\\Rset^{d_h}$.\n",
    "\n",
    "The Hessian matrix of the cost function is\n",
    "\n",
    "$$\n",
    "   \\frac{d^2 }{d\\vect{\\theta}^2} C(\\vect{\\theta}) \n",
    "   = B^{-1}  + J(\\vect{\\theta})^T R^{-1} J(\\vect{\\theta})\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta}\\in\\Rset^{d_h}$.\n",
    "\n",
    "If the covariance matrix $B$ is positive definite, then the\n",
    "Hessian matrix of the cost function is positive definite. Under this\n",
    "hypothesis, the solution of the nonlinear gaussian calibration is\n",
    "unique.\n",
    "\n",
    "## Solving the Non Linear Gaussian Calibration Problem\n",
    "\n",
    "The implementation of the resolution of the gaussian non linear\n",
    "calibration problem involves the Cholesky decomposition of the\n",
    "covariance matrices $B$ and $R$. This allows to transform\n",
    "the sum of two Mahalanobis distances into a single euclidian norm. This\n",
    "leads to a classical non linear least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Gaussian Calibration : bayesian BLUE\n",
    "\n",
    "We make the hypothesis that $h$ is linear with respect to\n",
    "$\\vect{\\theta}$, i.e., for any\n",
    "$\\vect{\\theta}\\in\\Rset^{d_h}$, we have :\n",
    "\n",
    "$$\n",
    "h(\\vect{\\theta}) = h(\\vect{\\mu}) + J(\\vect{\\theta}-\\vect{\\mu} ),\n",
    "$$\n",
    "\n",
    "where $J$ is the constant Jacobian matrix of $h$.\n",
    "\n",
    "Let $A$ be the matrix:\n",
    "\n",
    "$$\n",
    "A^{-1} = B^{-1} + J^T R^{-1} J.\n",
    "$$\n",
    "\n",
    "We denote by $K$ the Kalman matrix:\n",
    "\n",
    "$$\n",
    "K = A J^T R^{-1}.\n",
    "$$\n",
    "\n",
    "The maximum of the posterior distribution of $\\vect{\\theta}$ given\n",
    "the observations $\\vect{y}$ is:\n",
    "\n",
    "$$\n",
    "\\hat{\\vect{\\theta}} = \\vect{\\mu} + K (\\vect{y} - H(\\vect{\\mu})).\n",
    "$$\n",
    "\n",
    "It can be proved that:\n",
    "\n",
    "$$\n",
    "   p(\\vect{\\theta} | \\vect{y}) \\propto \n",
    "   \\exp\\left(\\frac{1}{2} (\\vect{\\theta} - \\hat{\\vect{\\theta}})^T A^{-1} (\\vect{\\theta} - \\hat{\\vect{\\theta}}) \\right)\n",
    "$$\n",
    "\n",
    "for any $\\vect{\\theta}\\in\\Rset^{d_h}$.\n",
    "\n",
    "This implies:\n",
    "\n",
    "$$\n",
    "\\hat{\\vect{\\theta}} \\sim \\mathcal{N}(\\vect{\\theta},A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "   - N. H. Bingham and John M. Fry (2010). *Regression, Linear Models in Statistics*, Springer Undergraduate Mathematics Series. Springer.\n",
    "\n",
    "   -  S. Huet, A. Bouvier, M.A. Poursat, and E. Jolivet (2004). *Statistical Tools for Nonlinear Regression*, Springer.\n",
    "\n",
    "   -  C.E. Rasmussen and C. K. I. Williams (2006), *Gaussian Processes for Machine Learning*, The MIT Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
