{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian calibration\n",
    "\n",
    "## References\n",
    "\n",
    "   - N. H. Bingham and John M. Fry (2010). *Regression, Linear Models in Statistics*, Springer Undergraduate Mathematics Series. Springer.\n",
    "\n",
    "   -  S. Huet, A. Bouvier, M.A. Poursat, and E. Jolivet (2004). *Statistical Tools for Nonlinear Regression*, Springer.\n",
    "\n",
    "   -  C.E. Rasmussen and C. K. I. Williams (2006), *Gaussian Processes for Machine Learning*, The MIT Press.\n",
    "   -  Mark Asch, Marc Bocquet, MaÃ«lle Nodet. \"Data assimilation: methods, algorithms, and applications\". SIAM. (2016)\n",
    "   -  Inverse Problem Theory and Model Parameter Estimation. Albert Tarantola, SIAM, 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We consider a computer model $\\boldsymbol{h}$ (i.e. a deterministic\n",
    "function) to calibrate:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z} = \\boldsymbol{h}(\\boldsymbol{x}, \\boldsymbol{\\theta}),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "-  $\\boldsymbol{x} \\in \\mathbb{R}^{d_x}$ is the input vector;\n",
    "-  $\\boldsymbol{z} \\in \\mathbb{R}^{d_z}$ is the output vector;\n",
    "-  $\\boldsymbol{\\theta} \\in \\mathbb{R}^{d_h}$ are the unknown parameters of\n",
    "   $\\boldsymbol{h}$ to calibrate.\n",
    "\n",
    "Let $n \\in \\mathbb{N}$ be the number of observations. The standard\n",
    "hypothesis of the probabilistic calibration is:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{Y}^i = \\boldsymbol{z}^i + \\boldsymbol{\\varepsilon}^i,\n",
    "$$\n",
    "\n",
    "for $i=1,...,n$ where $\\boldsymbol{\\varepsilon}^i$ is a random\n",
    "measurement error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goal of Gaussian calibration is to estimate $\\boldsymbol{\\theta}$,\n",
    "based on observations of $n$ inputs\n",
    "$(\\boldsymbol{x}^1, \\ldots, \\boldsymbol{x}^n)$ and the associated $n$\n",
    "observations of the output $(\\boldsymbol{y}^1, \\ldots, \\boldsymbol{y}^n)$. In\n",
    "other words, the calibration process reduces the discrepancy between the\n",
    "observations $(\\boldsymbol{y}^1, \\ldots, \\boldsymbol{y}^n)$ and the\n",
    "predictions $\\boldsymbol{h}(\\boldsymbol{\\theta})$. Given that\n",
    "$(\\boldsymbol{y}^1, \\ldots, \\boldsymbol{y}^n)$ are realizations of a random\n",
    "variable, the estimate of $\\boldsymbol{\\theta}$, denoted by\n",
    "$\\hat{\\boldsymbol{\\theta}}$, is also a random variable. Hence, the\n",
    "secondary goal of calibration is to estimate the distribution of\n",
    "$\\hat{\\boldsymbol{\\theta}}$ representing the uncertainty of the\n",
    "calibration process.\n",
    "\n",
    "In the remaining of this section, the input $\\boldsymbol{x}$ is not\n",
    "involved anymore in the equations. This is why we simplify the equation\n",
    "into:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z} = \\boldsymbol{h}(\\boldsymbol{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian calibration\n",
    "\n",
    "The bayesian calibration framework is based on two hypotheses.\n",
    "\n",
    "The first hypothesis is that the parameter $\\boldsymbol{\\theta}$ has a\n",
    "known distribution, called the *prior* distribution, and denoted by\n",
    "$p(\\boldsymbol{\\theta})$.\n",
    "\n",
    "The second hypothesis is that the output observations\n",
    "$(\\boldsymbol{y}^1, \\ldots, \\boldsymbol{y}^n)$ are sampled from a known\n",
    "conditional distribution denoted by $p(\\boldsymbol{y} | \\boldsymbol{\\theta})$.\n",
    "\n",
    "For any $\\boldsymbol{y}\\in\\mathbb{R}^{d_z}$ such that $p(\\boldsymbol{y})>0$,\n",
    "the Bayes theorem implies that the conditional distribution of\n",
    "$\\boldsymbol{\\theta}$ given $\\boldsymbol{y}$ is:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta} | \\boldsymbol{y}) = \\frac{p(\\boldsymbol{y} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})}{p(\\boldsymbol{y})}\n",
    "$$\n",
    "\n",
    "for any $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d_h}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The denominator of the previous Bayes fraction is independent of\n",
    "$\\boldsymbol{\\theta}$, so that the posterior distribution is\n",
    "proportional to the numerator:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta} | \\boldsymbol{y}) \\propto  p(\\boldsymbol{y} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "for any $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d_h}$.\n",
    "\n",
    "In the Gaussian calibration, the two previous distributions are assumed\n",
    "to be Gaussian.\n",
    "\n",
    "More precisely, we make the hypothesis that the parameter\n",
    "$\\boldsymbol{\\theta}$ has the Gaussian distribution:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, B),\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mu}\\in\\mathbb{R}^{d_h}$ is the mean of the Gaussian prior\n",
    "distribution, which is named the *background* and\n",
    "$B\\in\\mathbb{R}^{d_h \\times d_h}$ is the covariance matrix of the\n",
    "parameter.\n",
    "\n",
    "Secondly, we make the hypothesis that the output observations have the\n",
    "conditional gaussian distribution:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} | \\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{h}(\\boldsymbol{\\theta}), R),\n",
    "$$\n",
    "\n",
    "where $R\\in\\mathbb{R}^{d_z \\times d_z}$ is the covariance matrix of the\n",
    "output observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior distribution\n",
    "\n",
    "Denote by $\\|\\cdot\\|_B$ the Mahalanobis distance associated with\n",
    "the matrix $B$ :\n",
    "\n",
    "$$\n",
    "\\|\\boldsymbol{\\theta}-\\boldsymbol{\\mu} \\|^2_B = (\\boldsymbol{\\theta}-\\boldsymbol{\\mu} )^T B^{-1} (\\boldsymbol{\\theta}-\\boldsymbol{\\mu} ),\n",
    "$$\n",
    "\n",
    "for any $\\boldsymbol{\\theta},\\boldsymbol{\\mu} \\in \\mathbb{R}^{d_h}$. Denote by\n",
    "$\\|\\cdot\\|_R$ the Mahalanobis distance associated with the matrix\n",
    "$R$ :\n",
    "\n",
    "$$\n",
    "\\|\\boldsymbol{y}-H(\\boldsymbol{\\theta})\\|^2_R = (\\boldsymbol{y}-H(\\boldsymbol{\\theta}))^T R^{-1} (\\boldsymbol{y}-H(\\boldsymbol{\\theta})).\n",
    "$$\n",
    "\n",
    "for any $\\boldsymbol{\\theta} \\in \\mathbb{R}^{d_h}$ and any\n",
    "$\\boldsymbol{y} \\in \\mathbb{R}^{d_z}$. Therefore, the posterior distribution\n",
    "of $\\boldsymbol{\\theta}$ given the observations $\\boldsymbol{y}$ is :\n",
    "\n",
    "$$\n",
    "   p(\\boldsymbol{\\theta}|\\boldsymbol{y}) \\propto \\exp\\left( -\\frac{1}{2} \\left( \\|\\boldsymbol{y}-H(\\boldsymbol{\\theta})\\|^2_R \n",
    "   + \\|\\boldsymbol{\\theta}-\\boldsymbol{\\mu} \\|^2_B \\right) \\right)\n",
    "$$\n",
    "\n",
    "for any $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d_h}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MAP estimator\n",
    "\n",
    "The maximum of the posterior distribution of $\\boldsymbol{\\theta}$ given\n",
    "the observations $\\boldsymbol{y}$ is reached at :\n",
    "\n",
    "$$\n",
    "   \\hat{\\boldsymbol{\\theta}} = \\textrm{argmin}_{\\boldsymbol{\\theta}\\in\\mathbb{R}^{d_h}} \\frac{1}{2} \\left( \\|\\boldsymbol{y} - H(\\boldsymbol{\\theta})\\|^2_R \n",
    "   + \\|\\boldsymbol{\\theta}-\\boldsymbol{\\mu} \\|^2_B \\right).\n",
    "$$\n",
    "\n",
    "It is called the *maximum a posteriori posterior* estimator or *MAP*\n",
    "estimator.\n",
    "\n",
    "## Regularity of solutions of the Gaussian Calibration\n",
    "\n",
    "The gaussian calibration is a tradeoff, so that the second expression\n",
    "acts as a *spring* which pulls the parameter $\\boldsymbol{\\theta}$\n",
    "closer to the background $\\boldsymbol{\\mu}$ (depending on the \"spring\n",
    "constant\" $B$, meanwhile getting as close a possible to the\n",
    "observations. Depending on the matrix $B$, the computation may\n",
    "have better regularity properties than the plain non linear least\n",
    "squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non Linear Gaussian Calibration : 3DVAR\n",
    "\n",
    "The cost function of the gaussian nonlinear calibration problem is :\n",
    "\n",
    "$$\n",
    "   C(\\boldsymbol{\\theta}) = \\frac{1}{2}\\|\\boldsymbol{y}-H(\\boldsymbol{\\theta})\\|^2_R \n",
    "   + \\frac{1}{2}\\|\\boldsymbol{\\theta}-\\boldsymbol{\\mu} \\|^2_B\n",
    "$$\n",
    "\n",
    "for any $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d_h}$.\n",
    "\n",
    "The goal of the non linear gaussian calibration is to find the value of\n",
    "$\\boldsymbol{\\theta}$ which minimizes the cost function $C$. In\n",
    "general, this involves using a nonlinear unconstrained optimization\n",
    "solver.\n",
    "\n",
    "Let $J \\in \\mathbb{R}^{n \\times d_h}$ be the Jacobian matrix made of\n",
    "the partial derivatives of $\\boldsymbol{h}$ with respect to\n",
    "$\\boldsymbol{\\theta}$:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{\\theta}}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Jacobian matrix of the cost function $C$ can be expressed\n",
    "depending on the matrices $R$ and $B$ and the Jacobian\n",
    "matrix of the function $h$:\n",
    "\n",
    "$$\n",
    "   \\frac{d }{d\\boldsymbol{\\theta}} C(\\boldsymbol{\\theta}) \n",
    "   = B^{-1} (\\boldsymbol{\\theta}-\\boldsymbol{\\mu}) + J(\\boldsymbol{\\theta})^T R^{-1} (H(\\boldsymbol{\\theta}) - \\boldsymbol{y})\n",
    "$$\n",
    "\n",
    "for any $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d_h}$.\n",
    "\n",
    "The Hessian matrix of the cost function is\n",
    "\n",
    "$$\n",
    "   \\frac{d^2 }{d\\boldsymbol{\\theta}^2} C(\\boldsymbol{\\theta}) \n",
    "   = B^{-1}  + J(\\boldsymbol{\\theta})^T R^{-1} J(\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "for any $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d_h}$.\n",
    "\n",
    "If the covariance matrix $B$ is positive definite, then the\n",
    "Hessian matrix of the cost function is positive definite. Under this\n",
    "hypothesis, the solution of the nonlinear gaussian calibration is\n",
    "unique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving the Non Linear Gaussian Calibration Problem\n",
    "\n",
    "The implementation of the resolution of the gaussian non linear\n",
    "calibration problem involves the Cholesky decomposition of the\n",
    "covariance matrices $B$ and $R$. This allows to transform\n",
    "the sum of two Mahalanobis distances into a single euclidian norm. This\n",
    "leads to a classical non linear least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Gaussian Calibration : bayesian BLUE\n",
    "\n",
    "We make the hypothesis that $h$ is linear with respect to\n",
    "$\\boldsymbol{\\theta}$, i.e., for any\n",
    "$\\boldsymbol{\\theta}\\in\\mathbb{R}^{d_h}$, we have :\n",
    "\n",
    "$$\n",
    "h(\\boldsymbol{\\theta}) = h(\\boldsymbol{\\mu}) + J(\\boldsymbol{\\theta}-\\boldsymbol{\\mu} ),\n",
    "$$\n",
    "\n",
    "where $J$ is the constant Jacobian matrix of $h$.\n",
    "\n",
    "Let $A$ be the matrix:\n",
    "\n",
    "$$\n",
    "A^{-1} = B^{-1} + J^T R^{-1} J.\n",
    "$$\n",
    "\n",
    "We denote by $K$ the Kalman matrix:\n",
    "\n",
    "$$\n",
    "K = A J^T R^{-1}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The maximum of the posterior distribution of $\\boldsymbol{\\theta}$ given\n",
    "the observations $\\boldsymbol{y}$ is:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = \\boldsymbol{\\mu} + K (\\boldsymbol{y} - H(\\boldsymbol{\\mu})).\n",
    "$$\n",
    "\n",
    "It can be proved that:\n",
    "\n",
    "$$\n",
    "   p(\\boldsymbol{\\theta} | \\boldsymbol{y}) \\propto \n",
    "   \\exp\\left(\\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T A^{-1} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) \\right)\n",
    "$$\n",
    "\n",
    "for any $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d_h}$.\n",
    "\n",
    "This implies:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} \\sim \\mathcal{N}(\\boldsymbol{\\theta},A)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
