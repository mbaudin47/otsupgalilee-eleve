{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimer la tendance centrale\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Soit $\\boldsymbol{X} = (X_1,..., X_p)\\in\\mathbb{R}^p$ le vecteur aléatoire d’entrée. On note $f$ la densité de probabilité de la variable $\\boldsymbol{X}$. \n",
    "\n",
    "Soit $g$ une fonction de $\\mathbb{R}^p$ vers $\\mathbb{R}$. On souhaite estimer la tendance centrale de la variable aléatoire :\n",
    "\n",
    "$$\n",
    "Y = g(\\boldsymbol{X}).\n",
    "$$\n",
    "\n",
    "Note : pour les méthodes que nous allons étudier, le cas plus général où $Y\\in\\mathbb{R}^q$ se traite de manière similaire lorsqu'on ne prend pas en compte la dépendance entre les composantes du vecteur de sortie $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dans ce texte, nous considérons que l'analyse en tendance centrale consiste principalement à estimer la variabilité de $Y$ autour de sa moyenne. Dans ce but, nous allons estimer l'espérance de $Y$ :\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(Y) = \\int_{\\mathbb{R}^p} g(\\boldsymbol{x}) f(\\boldsymbol{x}) d\\boldsymbol{x}.\n",
    "$$\n",
    "\n",
    "De plus, nous allons estimer la variance de $Y$ :\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(Y) = \\mathbb{E}\\left[(Y - \\mathbb{E}(Y))^2\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bien sûr, il arrive que $X$ soit un vecteur de petite dimension, que les marginales de $\\boldsymbol{X}$ soient gaussiennes, indépendantes et que la fonction $g$ soit linéaire : nous exploiterons d'ailleurs cette dernière propriété par la suite. Toutefois, en général, la distribution de $Y$ est inconnue car le lien entre l'entrée $\\boldsymbol{X}$ et la sortie $Y$ est complexe :\n",
    "\n",
    "- d'une part, les marginales de $\\boldsymbol{X}$, sa structure de dépendance et sa dimension peuvent induire un vecteur aléatoire $\\boldsymbol{X}$ complexe,\n",
    "- d'autre part, les méthodes que nous allons utiliser considèrent que la fonction $g$ est considérée comme une *boîte noire* dont la structure interne est à priori inconnue et ne peut être exploitée.\n",
    "\n",
    "C'est la raison pour laquelle des méthodes d'estimation doivent être employées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumul quadratique (développement de Taylor)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Soit $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$ la moyenne du vecteur aléatoire $\\boldsymbol{X}$ :\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu} = \\mathbb{E}(\\boldsymbol{X}).\n",
    "$$\n",
    "\n",
    "Puisque la densité de probabilité $f$ est connue, le point $\\boldsymbol{\\mu}$ peut être calculé sans difficulté. \n",
    "Le développement de Taylor à l'ordre 2 de la fonction $g$ au point $\\boldsymbol{\\mu}$ implique :\n",
    "\n",
    "\\begin{align*}\n",
    "g(\\boldsymbol{X}) \n",
    "&= g(\\boldsymbol{\\mu}) + \\sum_{i=1}^p \\frac{\\partial g}{\\partial x_i} (\\boldsymbol{\\mu}) (X_i - \\mu_i) \\\\\n",
    "&+ \\frac{1}{2} \\sum_{i=1}^p \\sum_{j=1}^p \\frac{\\partial^2 g}{\\partial x_i \\partial x_j} (\\boldsymbol{\\mu}) \n",
    "  (X_i - \\mu_i) (X_j - \\mu_j)\n",
    "+ O(\\|\\boldsymbol{X} - \\boldsymbol{\\mu}\\|_2^3)\n",
    "\\end{align*}\n",
    "\n",
    "quand $\\boldsymbol{X} \\rightarrow \\boldsymbol{\\mu}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cumul quadratique avec dépendance\n",
    "\n",
    "Faisons l'hypothèse que le développement de Taylor à l'ordre 2 est exact. En d'autres termes, \n",
    "faisons l'hypothèse que :\n",
    "\n",
    "\\begin{align*}\n",
    "g(\\boldsymbol{X}) \n",
    "&= g(\\boldsymbol{\\mu}) + \\sum_{i=1}^p \\frac{\\partial g}{\\partial x_i} (\\boldsymbol{\\mu}) (X_i - \\mu_i) \\\\\n",
    "&+ \\frac{1}{2} \\sum_{i=1}^p \\sum_{j=1}^p \\frac{\\partial^2 g}{\\partial x_i \\partial x_j} (\\boldsymbol{\\mu}) \n",
    "  (X_i - \\mu_i) (X_j - \\mu_j).\n",
    "\\end{align*}\n",
    "\n",
    "La linéarité de l'espérance implique :\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(Y) &= \\mathbb{E}(g(\\boldsymbol{X})) \\\\\n",
    "&= g(\\boldsymbol{\\mu}) + \\sum_{i=1}^p \\frac{\\partial g}{\\partial x_i} (\\boldsymbol{\\mu}) \\mathbb{E}(X_i - \\mu_i) \\\\\n",
    "&+ \\frac{1}{2} \\sum_{i=1}^p \\sum_{j=1}^p \\frac{\\partial^2 g}{\\partial x_i \\partial x_j} (\\boldsymbol{\\mu}) \n",
    "  \\mathbb{E}((X_i - \\mu_i) (X_j - \\mu_j)).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "D'une part, la linéarité de l'espérance implique :\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(X_i - \\mu_i) = \\mathbb{E}(X_i) - \\mu_i = \\mu_i - \\mu_i = 0\n",
    "$$\n",
    "\n",
    "pour $i=1,...,p$.\n",
    "\n",
    "D'autre part, la définition de la covariance implique :\n",
    "\n",
    "$$\n",
    "\\mathbb{E}((X_i - \\mu_i) (X_j - \\mu_j)) = \\operatorname{Cov}(X_i, X_j),\n",
    "$$\n",
    "\n",
    "pour $i,j=1,...,p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Soit $\\sigma_i$ l'écart-type de la variable $X_i$, i.e.\n",
    "\n",
    "$$\n",
    "\\sigma_i = \\sqrt{\\operatorname{Var}(X_i)},\n",
    "$$\n",
    "\n",
    "pour $i=1,...,p$. \n",
    "\n",
    "Soit $\\rho_{i,j}$ le coefficient de corrélation :\n",
    "\n",
    "$$\n",
    "\\rho_{i,j} = \\frac{\\operatorname{Cov}(X_i,X_j)}{ \\sigma_i \\sigma_j}\n",
    "$$\n",
    "\n",
    "pour $i,j=1,...,p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alors $\\operatorname{Cov}(X_i,X_j) = \\sigma_i \\sigma_j \\rho_{i,j}$ pour $i,j=1,...,p$, ce qui implique :\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(Y)  \n",
    "&= g(\\boldsymbol{\\mu}) + \\frac{1}{2} \\sum_{i=1}^p \\sum_{j=1}^p \\frac{\\partial^2 g}{\\partial x_i \\partial x_j} (\\boldsymbol{\\mu}) \\sigma_i \\sigma_j \\rho_{i,j}.\n",
    "\\end{align*}\n",
    "\n",
    "Pour estimer la variance de $g(X)$, faisons l'hypothèse que le développement de Taylor de $g$ est exact à l'ordre 1. \n",
    "En d'autres termes, \n",
    "faisons l'hypothèse que :\n",
    "\n",
    "\\begin{align*}\n",
    "g(\\boldsymbol{X}) \n",
    "&= g(\\boldsymbol{\\mu}) + \\sum_{i=1}^p \\frac{\\partial g}{\\partial x_i} (\\boldsymbol{\\mu}) (X_i - \\mu_i).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cela implique :\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(g(\\boldsymbol{X})) &= g(\\boldsymbol{\\mu}).\n",
    "\\end{align*}\n",
    "\n",
    "Les propriétés de la variance impliquent :\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}(Y) &= \\operatorname{Var}(g(\\boldsymbol{X}))\\\\\n",
    "&= \\sum_{i=1}^p \\sum_{j=1}^p \\frac{\\partial g}{\\partial x_i} (\\boldsymbol{\\mu}) \\frac{\\partial g}{\\partial x_j} (\\boldsymbol{\\mu}) \\operatorname{Cov}((X_i - \\mu_i)(X_j - \\mu_j)).\n",
    "\\end{align*}\n",
    "\n",
    "Par conséquent, \n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}(Y) \n",
    "&= \\sum_{i=1}^p \\sum_{j=1}^p \\frac{\\partial g}{\\partial x_i} (\\boldsymbol{\\mu}) \\frac{\\partial g}{\\partial x_j} (\\boldsymbol{\\mu}) \\sigma_i \\sigma_j \\rho_{i,j}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cumul quadratique sans dépendance\n",
    "\n",
    "Si les variables $X_i$ sont indépendantes, alors $\\rho_{i,j}=0$ pour $i,j=1,...,p$ et $i\\neq j$. Dans ce cas, les formules précédentes se simplifient.\n",
    "\n",
    "Alors, à l'ordre 2, l'espérance de $Y$ est :\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(Y)  \n",
    "&= g(\\boldsymbol{\\mu}) + \\frac{1}{2} \\sum_{i=1}^p \\frac{\\partial^2 g}{\\partial x_i^2} (\\boldsymbol{\\mu}) \\sigma_i^2.\n",
    "\\end{align*}\n",
    "\n",
    "De plus, à l'ordre 1, la variance de $Y$ est :\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}(Y) \n",
    "&= \\sum_{i=1}^p \\left(\\frac{\\partial g}{\\partial x_i} (\\boldsymbol{\\mu})\\right)^2 \\sigma_i^2.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sous cette hypothèse, la variance se décompose sous la forme d'une somme impliquant uniquement la variance de la variable $X_i$. C'est la raison pour laquelle on introduit les facteurs d'importance $\\eta_i$ :\n",
    "\n",
    "$$\n",
    "\\eta_i = \\frac{\\left(\\frac{\\partial g}{\\partial x_i} (\\boldsymbol{\\mu})\\right)^2 \\sigma_i^2}{\\operatorname{Var}(Y)}\n",
    "$$\n",
    "\n",
    "pour $i=1,...,p$. \n",
    "\n",
    "La décomposition de la variance implique :\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^p \\eta_i = 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cumul quadratique pour un modèle linéaire sans dépendance\n",
    "\n",
    "Supposons que $g$ est linéaire et que le vecteur $X$ est associé à des marginales indépendantes.\n",
    "\n",
    "Pour $i=1,...,p$, une valeur de $\\eta_i$ proche de zéro permet d'identifier une variable qui peut être remplacée par une constante sans changer significativement la variance. En effet, le facteur $\\eta_i$ quantifie la part de variance expliquée par la variable $X_i$, puisque :\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(Y) = \\sum_{i=1}^p \\eta_i \\operatorname{Var}(Y).\n",
    "$$\n",
    "\n",
    "Par conséquent, si on fixe la variable $X_i$ à sa valeur moyenne $\\mu_i$, alors la variance de la sortie est égale à \n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(Y|X_i=\\mu_i) = \\operatorname{Var}(Y)- \\eta_i \\operatorname{Var}(Y).\n",
    "$$\n",
    "\n",
    "Par conséquent, un facteur d'importance $\\eta_i$ proche de zéro implique que la variance ne va pas beaucoup être réduite si on fixe $X_i$ à sa valeur moyenne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Synthèse\n",
    "\n",
    "La méthode du cumul quadratique peut être appliquée lorsque la fonction $g$ peut être approchée par une fonction linéaire ou quadratique avec ou sans dépendance. L'espérance peut être calculée que la fonction $g$ soit linéaire ou quadratique, mais la variance ne peut être calculée que si $g$ est linéaire (pas quadratique). Les facteurs d'importance ne peuvent être calculés que si le vecteur d'entrée $X$ est à marginales indépendantes. La table suivante présente ces éléments du plus simple au plus complexe.\n",
    "\n",
    "| Vecteur d'entrée $X$ | Fonction $g$ | Espérance | Variance | Facteurs d'importance |\n",
    "|--|--|--|--|--|\n",
    "| Indépendant | Linéaire | Exacte | Exacte | ✓ |\n",
    "| Indépendant | Quadratique | Exacte | - | - |\n",
    "| Dépendant | Linéaire | Exacte | Exacte | - |\n",
    "| Dépendant | Quadratique | Exacte | - | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Il s'avère que cette méthode peut être utilisée plus souvent qu'on ne le pense à priori car il faut tenir compte à la fois de la distribution du vecteur aléatoire $X$ et de la fonction $g$. En effet, n'importe quelle fonction est *localement* linéaire ou quadratique. Par conséquent, même si la fonction $g$ est *très* non-linéaire, il suffit de considérer des distributions suffisamment étroites pour obtenir un comportement linéaire *compte tenu de la plage de variation des entrées* $X$. C'est une situation que l'on observe par exemple lorsqu'on considère des lois marginales gaussiennes associées à un coefficient de variation $\\sigma/\\mu$ inférieur ou égal à 10%. C'est en effet une situation fréquente lorsque les variables d'entrée sont des variables physiques dont la loi de distribution représente l'erreur de mesure.\n",
    "\n",
    "## Références\n",
    "\n",
    "- http://openturns.github.io/openturns/master/theory/reliability_sensitivity/taylor_moments.html\n",
    "- http://openturns.github.io/openturns/master/theory/reliability_sensitivity/taylor_importance_factors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Méthode de Monte-Carlo\n",
    "\n",
    "Soit $n$ un entier représentant la taille de l'échantillon. Soit $\\left\\{X^{(j)}\\right\\}_{j=1,...,n}$ un échantillon i.i.d. du vecteur aléatoire $X$. \n",
    "Soit \n",
    "\n",
    "$$\n",
    "Y^{(j)} = g\\left(X^{(j)}\\right)\n",
    "$$\n",
    "\n",
    "pour $j=1,...,n$. \n",
    "\n",
    "L'estimateur Monte-Carlo de la moyenne empirique est :\n",
    "\n",
    "$$\n",
    "\\bar{Y} = \\frac{1}{n} \\sum_{j=1}^n Y^{(j)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La linéarité de l'espérance implique :\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\left(\\bar{Y}\\right)\n",
    "&= \\frac{1}{n} E\\left( \\sum_{j=1}^n Y^{(j)}\\right) \\\\\n",
    "&= \\frac{1}{n} \\sum_{j=1}^n E\\left(Y^{(j)}\\right) \\\\\n",
    "&= \\frac{1}{n} \\sum_{j=1}^n \\mathbb{E}(Y).\n",
    "\\end{align*}\n",
    "\n",
    "Cela implique que cet estimateur est non biaisé :\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\left(\\bar{Y}\\right)\n",
    "&= \\mathbb{E}(Y).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L'indépendance des réalisations de l'échantillon implique :\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}\\left(\\bar{Y}\\right)\n",
    "&= \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{j=1}^n Y^{(j)} \\right) \\\\\n",
    "&= \\frac{1}{n^2} \\sum_{j=1}^n \\operatorname{Var}\\left(Y^{(j)}\\right) \\\\\n",
    "&= \\frac{1}{n^2} \\sum_{j=1}^n \\operatorname{Var}(Y) \\\\\n",
    "&= \\frac{1}{n^2} n \\operatorname{Var}(Y),\n",
    "\\end{align*}\n",
    "\n",
    "ce qui implique :\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}\\left(\\bar{Y}\\right)\n",
    "&= \\frac{1}{n} \\operatorname{Var}(Y).\n",
    "\\end{align*}\n",
    "\n",
    "Un estimateur non biaisé de la variance est \n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-1} \\sum_{j=1}^n \\left(Y^{(j)} - \\bar{Y}\\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Supposons que la variance $\\operatorname{Var}(Y)$ existe et est finie. \n",
    "Le théorème central limite (théorème de Lindeberg–Lévy) indique que la distribution de la moyenne empirique $\\bar{Y}$ converge en loi vers la loi gaussienne. \n",
    "Plus précisément, \n",
    "\n",
    "$$\n",
    "\\sqrt{n} \\left(\\bar{Y} - \\mathbb{E}(Y)\\right) \\xrightarrow{d} \\mathcal{N}(0,\\operatorname{Var}(Y)).\n",
    "$$\n",
    "\n",
    "Cela implique :\n",
    "\n",
    "$$\n",
    "\\bar{Y} \\xrightarrow{d} \\mathcal{N}\\left(\\mathbb{E}(Y),\\frac{\\operatorname{Var}(Y)}{n}\\right).\n",
    "$$\n",
    "\n",
    "C'est pourquoi on peut dire que, de manière approchée, la précision absolue d'estimation de la moyenne dépend de $\\frac{\\sqrt{\\operatorname{Var}(Y)}}{\\sqrt{n}}$. Par exemple, si $\\operatorname{Var}(Y)=1$, alors il faut $n=10^4$ simulations pour obtenir une erreur absolue de l'ordre de $\\frac{1}{10^2} = 0.01$. D'un autre point de vue, pour réduire d'un facteur 2 l'erreur absolue d'estimation, il faut multiplier par 4 la taille de l'échantillon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Utilisons un algorithme itératif tel que la taille de l'échantillon $n$ est choisie en fonction d'une précision pré-déterminée. On peut utiliser:\n",
    "* un critère relatif fondé sur le coefficient de variation, \n",
    "* un critère absolu fondé sur l'écart-type.\n",
    "\n",
    "Lorsque l'espérance de la sortie est non nulle (c'est le cas général), on devrait plutôt utiliser le critère fondé sur le coefficient de variation, car il assure (sans être toutefois déterministe) un nombre de chiffres corrects dans l'estimation de l'espérance. \n",
    "\n",
    "On peut utiliser un critère absolu sur $Y$, fondé sur l'écart-type de $\\overline{Y}$:\n",
    "$$\n",
    "\\hat{\\sigma}\\left(\\overline{Y}\\right) = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\n",
    "$$\n",
    "On reconnaît dans l'expression précédente la convergence de l'algorithme de Monte-Carlo en $1 / \\sqrt{n}$.\n",
    "On peut stopper l'algorithme lorsque \n",
    "$$\n",
    "\\hat{\\sigma}\\left(\\overline{Y}\\right) < \\sigma_{max}\n",
    "$$\n",
    "où $\\sigma_{max}$ est un critère fourni par l'utilisateur déterminant l'écart-type maximal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Le coefficient de variation asymptotique de l'estimateur $\\overline{Y_n}$ est, par définition :\n",
    "$$\n",
    "\\operatorname{CV}\\left(\\overline{Y}\\right) = \\frac{\\sqrt{\\operatorname{Var}\\left(\\overline{Y}\\right)}}{\\left|\\mathbb{E}\\left(\\overline{Y}\\right)\\right|}\n",
    " = \\frac{\\sqrt{\\operatorname{Var}\\left(Y\\right) / n}}{\\left|\\mathbb{E}\\left(Y\\right)\\right|}.\n",
    "$$\n",
    "On peut l'estimer par:\n",
    "$$\n",
    "\\widehat{\\operatorname{CV}}\\left(\\overline{Y}\\right) = \\frac{\\hat{\\sigma}/\\sqrt{n}}{|\\mu|}.\n",
    "$$\n",
    "On peut stopper l'algorithme lorsque \n",
    "$$\n",
    "\\widehat{\\operatorname{CV}}\\left(\\overline{Y}\\right) < \\operatorname{CV}_{max}\n",
    "$$\n",
    "où $\\operatorname{CV}_{max}$ est un critère fourni par l'utilisateur déterminant le coefficient de variation maximal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cette propriété permet d'obtenir un intervalle de confiance asymptotique \n",
    "approché pour l'espérance. \n",
    "\n",
    "Soit $\\alpha\\in[0,1/2]$ un niveau de confiance. \n",
    "Soit $z_{1 - \\alpha / 2} = -\\Phi^{-1}(1 - \\alpha / 2)$\n",
    "le quantile de niveau $1 - \\alpha / 2$ de la loi gaussienne standard.\n",
    "Par symétrie de la loi gaussienne, on a $z_{\\alpha / 2} = - z_{1 - \\alpha / 2}$. \n",
    "Par exemple, si $\\alpha = 0.05$, alors les quantiles correspondant à 95\\% de \n",
    "confiance sont $z_{\\alpha / 2} = z_{0.025} = -1.960$ \n",
    "et $z_{1 - \\alpha / 2} = z_{0.975} = 1.960$. \n",
    "\n",
    "Alors, lorsque $n$ est grand :\n",
    "\n",
    "$$\n",
    "P\\left(\n",
    "\\mathbb{E}(Y) \\in \\left[\\bar{Y} - z_{1 - \\alpha / 2} \\frac{\\hat{\\sigma}}{\\sqrt{n}} , \n",
    "\\bar{Y} + z_{1 - \\alpha / 2} \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\right] \\right) = 1 - \\alpha.\n",
    "$$\n",
    "\n",
    "Cet intervalle est asymptotique, car il n'est vrai que lorsque $n$ est grand. Il est approché, car nous avons remplacé l'écart-type exact $\\sigma_Y$ par l'écart-type empirique $\\hat{\\sigma}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Références\n",
    "\n",
    "- http://openturns.github.io/openturns/master/user_manual/_generated/openturns.ExpectationSimulationAlgorithm.html\n",
    "- http://openturns.github.io/openturns/master/auto_reliability_sensitivity/central_dispersion/plot_central_tendency.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
