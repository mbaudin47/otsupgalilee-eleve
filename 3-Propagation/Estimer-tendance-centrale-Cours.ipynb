{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimer la tendance centrale\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Soit $X = (X_1,..., X_d)\\in\\mathbb{R}^d$ le vecteur aléatoire d’entrée. On note $f$ la densité de probabilité de la variable $X$. \n",
    "\n",
    "Soit $g$ une fonction de $\\mathbb{R}^d$ vers $\\mathbb{R}$. On souhaite estimer la variable aléatoire :\n",
    "$$\n",
    "Y = g(X).\n",
    "$$\n",
    "\n",
    "Note : pour les méthodes que nous allons étudier, le cas plus général où $Y\\in\\mathbb{R}^p$ se traite de manière similaire lorsqu'on ne prend pas en compte la dépendance entre les composantes du vecteur de sortie $Y$. \n",
    "\n",
    "Dans ce texte, nous considérons que l'analyse en tendance centrale consiste principalement à estimer la variabilité de $Y$ autour de sa moyenne. Dans ce but, nous allons estimer l'espérance de $Y$ :\n",
    "$$\n",
    "E(Y) = \\int_{\\mathbb{R}^d} g(x) f(x) dx.\n",
    "$$\n",
    "De plus, nous allons estimer la variance de $Y$ :\n",
    "$$\n",
    "V(Y) = E\\left[(Y-E(Y))^2\\right].\n",
    "$$\n",
    "\n",
    "Bien sûr, il arrive que $X$ soit un vecteur de petite dimension, que les marginales de $X$ soient gaussiennes, indépendantes et que la fonction $g$ soit linéaire : nous exploiterons d'ailleurs cette dernière propriété par la suite. Toutefois, en général, la distribution de $Y$ est inconnue car le lien entre l'entrée $X$ et la sortie $Y$ est complexe :\n",
    "- d'une part, les marginales de $X$, sa structure de dépendance et sa dimension peuvent induire un vecteur aléatoire $X$ complexe,\n",
    "- d'autre part, les méthodes que nous allons utiliser considèrent que la fonction $g$ est considérée comme une *boîte noire* dont la structure interne est à priori inconnue et ne peut être exploitée.\n",
    "\n",
    "C'est la raison pour laquelle des méthodes d'estimation doivent être employées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumul quadratique (développement de Taylor)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Soit $\\mu\\in\\mathbb{R}^d$ la moyenne du vecteur aléatoire $X$ :\n",
    "$$\n",
    "\\mu = E(X).\n",
    "$$\n",
    "Puisque la densité de probabilité $f$ est connue, le point $\\mu$ peut être calculé sans difficulté. \n",
    "Le développement de Taylor à l'ordre 2 de la fonction $g$ au point $\\mu$ implique :\n",
    "\\begin{align}\n",
    "g(X) \n",
    "&= g(\\mu) + \\sum_{i=1}^d \\frac{\\partial g}{\\partial X_i} (\\mu) (X_i - \\mu_i) \\\\\n",
    "&+ \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\frac{\\partial^2 g}{\\partial X_i \\partial X_j} (\\mu) \n",
    "  (X_i - \\mu_i) (X_j - \\mu_j)\n",
    "+ O(\\|X-\\mu\\|_2^3)\n",
    "\\end{align}\n",
    "quand $X\\rightarrow \\mu$. \n",
    "\n",
    "### Cumul quadratique avec dépendance\n",
    "\n",
    "Faisons l'hypothèse que le développement de Taylor à l'ordre 2 est exact. En d'autres termes, \n",
    "faisons l'hypothèse que :\n",
    "\\begin{align}\n",
    "g(X) \n",
    "&= g(\\mu) + \\sum_{i=1}^d \\frac{\\partial g}{\\partial X_i} (\\mu) (X_i - \\mu_i) \\\\\n",
    "&+ \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\frac{\\partial^2 g}{\\partial X_i \\partial X_j} (\\mu) \n",
    "  (X_i - \\mu_i) (X_j - \\mu_j).\n",
    "\\end{align}\n",
    "La linéarité de l'espérance implique :\n",
    "\\begin{align}\n",
    "E(Y) &= E(g(X)) \\\\\n",
    "&= g(\\mu) + \\sum_{i=1}^d \\frac{\\partial g}{\\partial X_i} (\\mu) E(X_i - \\mu_i) \\\\\n",
    "&+ \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\frac{\\partial^2 g}{\\partial X_i \\partial X_j} (\\mu) \n",
    "  E((X_i - \\mu_i) (X_j - \\mu_j)).\n",
    "\\end{align}\n",
    "D'une part, la linéarité de l'espérance implique :\n",
    "\\begin{align}\n",
    "E(X_i - \\mu_i) = E(X_i) - \\mu_i = \\mu_i - \\mu_i = 0\n",
    "\\end{align}\n",
    "pour $i=1,...,d$.\n",
    "D'autre part, la définition de la covariance implique :\n",
    "\\begin{align}\n",
    "E((X_i - \\mu_i) (X_j - \\mu_j))\n",
    "&=Cov(X_i,X_j),\n",
    "\\end{align}\n",
    "pour $i,j=1,...,d$. \n",
    "Soit $\\sigma_i$ l'écart-type de la variable $X_i$, i.e.\n",
    "$$\n",
    "\\sigma_i = \\sqrt{V(X_i)},\n",
    "$$\n",
    "pour $i=1,...,d$. \n",
    "Soit $\\rho_{i,j}$ le coefficient de corrélation :\n",
    "$$\n",
    "\\rho_{i,j} = \\frac{Cov(X_i,X_j)}{ \\sigma_i \\sigma_j}\n",
    "$$\n",
    "pour $i,j=1,...,d$. \n",
    "Alors $Cov(X_i,X_j) = \\sigma_i \\sigma_j \\rho_{i,j}$ pour $i,j=1,...,d$, ce qui implique :\n",
    "\\begin{align}\n",
    "E(Y)  \n",
    "&= g(\\mu) + \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\frac{\\partial^2 g}{\\partial X_i \\partial X_j} (\\mu) \\sigma_i \\sigma_j \\rho_{i,j}.\n",
    "\\end{align}\n",
    "\n",
    "Pour estimer la variance de $g(X)$, faisons l'hypothèse que le développement de Taylor de $g$ est exact à l'ordre 1. \n",
    "En d'autres termes, \n",
    "faisons l'hypothèse que :\n",
    "\\begin{align}\n",
    "g(X) \n",
    "&= g(\\mu) + \\sum_{i=1}^d \\frac{\\partial g}{\\partial X_i} (\\mu) (X_i - \\mu_i).\n",
    "\\end{align}\n",
    "Cela implique :\n",
    "\\begin{align}\n",
    "E(g(X)) &= g(\\mu).\n",
    "\\end{align}\n",
    "Les propriétés de la variance impliquent :\n",
    "\\begin{align}\n",
    "V(Y) &= V(g(X))\\\\\n",
    "&= \\sum_{i=1}^d \\sum_{j=1}^d \\frac{\\partial g}{\\partial X_i} (\\mu) \\frac{\\partial g}{\\partial X_j} (\\mu) Cov((X_i - \\mu_i)(X_j - \\mu_j)).\n",
    "\\end{align}\n",
    "Par conséquent, \n",
    "\\begin{align}\n",
    "V(Y) \n",
    "&= \\sum_{i=1}^d \\sum_{j=1}^d \\frac{\\partial g}{\\partial X_i} (\\mu) \\frac{\\partial g}{\\partial X_j} (\\mu) \\sigma_i \\sigma_j \\rho_{i,j}.\n",
    "\\end{align}\n",
    "\n",
    "### Cumul quadratique sans dépendance\n",
    "\n",
    "Si les variables $X_i$ sont indépendantes, alors $\\rho_{i,j}=0$ pour $i,j=1,...,d$. Dans ce cas, les formules précédentes se simplifient.\n",
    "\n",
    "Alors, à l'ordre 2, l'espérance de $Y$ est :\n",
    "\\begin{align}\n",
    "E(Y)  \n",
    "&= g(\\mu) + \\frac{1}{2} \\sum_{i=1}^d \\frac{\\partial^2 g}{\\partial X_i^2} (\\mu) \\sigma_i^2.\n",
    "\\end{align}\n",
    "De plus, à l'ordre 1, la variance de $Y$ est :\n",
    "\\begin{align}\n",
    "V(Y) \n",
    "&= \\sum_{i=1}^d \\left(\\frac{\\partial g}{\\partial X_i} (\\mu)\\right)^2 \\sigma_i^2.\n",
    "\\end{align}\n",
    "\n",
    "Sous cette hypothèse, la variance se décompose sous la forme d'une somme impliquant uniquement la variance de la variable $X_i$. C'est la raison pour laquelle on introduit les facteurs d'importance $\\eta_i$ :\n",
    "$$\n",
    "\\eta_i = \\frac{\\left(\\frac{\\partial g}{\\partial X_i} (\\mu)\\right)^2 \\sigma_i^2}{V(Y)}\n",
    "$$\n",
    "pour $i=1,...,d$. \n",
    "La décomposition de la variance implique :\n",
    "$$\n",
    "\\sum_{i=1}^d \\eta_i = 1.\n",
    "$$\n",
    "\n",
    "### Cumul quadratique pour un modèle linéaire sans dépendance\n",
    "\n",
    "Supposons que $g$ est linéaire et que le vecteur $X$ est associé à des marginales indépendantes.\n",
    "\n",
    "Pour $i=1,...,d$, une valeur de $\\eta_i$ proche de zéro permet d'identifier une variable qui peut être remplacée par une constante sans changer significativement la variance. En effet, le facteur $\\eta_i$ quantifie la part de variance expliquée par la variable $X_i$, puisque :\n",
    "$$\n",
    "V(Y) = \\sum_{i=1}^d \\eta_i V(Y).\n",
    "$$\n",
    "Par conséquent, si on fixe la variable $X_i$ à sa valeur moyenne $\\mu_i$, alors la variance de la sortie est égale à \n",
    "$$\n",
    "V(Y|X=\\mu_i) = V(Y)- \\eta_i V(Y).\n",
    "$$\n",
    "Par conséquent, un facteur d'importance $\\eta_i$ proche de zéro implique que la variance ne va pas beaucoup être réduite si on fixe $X_i$ à sa valeur moyenne.\n",
    "\n",
    "### Synthèse\n",
    "\n",
    "La méthode du cumul quadratique peut être appliquée lorsque la fonction $g$ peut être approchée par une fonction linéaire ou quadratique avec ou sans dépendance. L'espérance peut être calculée que la fonction $g$ soit linéaire ou quadratique, mais la variance ne peut être calculée que si $g$ est linéaire (pas quadratique). Les facteurs d'importance ne peuvent être calculés que si le vecteur d'entrée $X$ est à marginales indépendantes. La table suivante présente ces éléments du plus simple au plus complexe.\n",
    "\n",
    "| Vecteur d'entrée $X$ | Fonction $g$ | Espérance | Variance | Facteurs d'importance |\n",
    "|--|--|--|--|--|\n",
    "| Indépendant | Linéaire | Exacte | Exacte | ✓ |\n",
    "| Indépendant | Quadratique | Exacte | - | - |\n",
    "| Dépendant | Linéaire | Exacte | Exacte | - |\n",
    "| Dépendant | Quadratique | Exacte | - | - |\n",
    "\n",
    "Il s'avère que cette méthode peut être utilisée plus souvent qu'on ne le pense à priori car il faut tenir compte à la fois de la distribution du vecteur aléatoire $X$ et de la fonction $g$. En effet, n'importe quelle fonction est *localement* linéaire ou quadratique. Par conséquent, même si la fonction $g$ est *très* non-linéaire, il suffit de considérer des distributions suffisamment étroites pour obtenir un comportement linéaire *compte tenu de la plage de variation des entrées* $X$. C'est une situation que l'on observe par exemple lorsqu'on considère des lois marginales gaussiennes associées à un coefficient de variation $\\sigma/\\mu$ inférieur ou égal à 10%. C'est en effet une situation fréquente lorsque les variables d'entrée sont des variables physiques dont la loi de distribution représente l'erreur de mesure.\n",
    "\n",
    "## Références\n",
    "\n",
    "- http://openturns.github.io/openturns/master/theory/reliability_sensitivity/taylor_moments.html\n",
    "- http://openturns.github.io/openturns/master/theory/reliability_sensitivity/taylor_importance_factors.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de Monte-Carlo\n",
    "\n",
    "Soit $n$ un entier représentant la taille de l'échantillon. Soit $\\left\\{X_i^{(j)}\\right\\}_{i=1,...,n}$ un échantillon i.i.d. du vecteur aléatoire $X$. \n",
    "Soit \n",
    "$$\n",
    "Y^{(j)} = g\\left(X_i^{(j)}\\right)\n",
    "$$\n",
    "pour $j=1,...,n$. \n",
    "L'estimateur Monte-Carlo de la moyenne empirique est :\n",
    "$$\n",
    "\\bar{Y} = \\frac{1}{n} \\sum_{j=1}^n Y^{(j)}.\n",
    "$$\n",
    "\n",
    "La linéarité de l'espérance implique :\n",
    "\\begin{align}\n",
    "E\\left(\\bar{Y}\\right)\n",
    "&= \\frac{1}{n} E\\left( \\sum_{j=1}^n Y^{(j)}\\right) \\\\\n",
    "&= \\frac{1}{n} \\sum_{j=1}^n E\\left(Y^{(j)}\\right) \\\\\n",
    "&= \\frac{1}{n} \\sum_{j=1}^n E(g(X)).\n",
    "\\end{align}\n",
    "Cela implique que cet estimateur est non biaisé :\n",
    "\\begin{align}\n",
    "E\\left(\\bar{Y}\\right)\n",
    "&= E(g(X)).\n",
    "\\end{align}\n",
    "\n",
    "L'indépendance des réalisations de l'échantillon implique :\n",
    "\\begin{align}\n",
    "V\\left(\\bar{Y}\\right)\n",
    "&= \\frac{1}{n^2} V\\left(\\sum_{j=1}^n Y^{(j)} \\right) \\\\\n",
    "&= \\frac{1}{n^2} \\sum_{j=1}^n V\\left(Y^{(j)}\\right) \\\\\n",
    "&= \\frac{1}{n^2} \\sum_{j=1}^n V(Y) \\\\\n",
    "&= \\frac{1}{n^2} n V(Y),\n",
    "\\end{align}\n",
    "ce qui implique :\n",
    "\\begin{align}\n",
    "V\\left(\\bar{Y}\\right)\n",
    "&= \\frac{1}{n} V(Y).\n",
    "\\end{align}\n",
    "\n",
    "Un estimateur non biaisé de la variance est \n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-1} \\sum_{j=1}^n \\left(Y^{(j)} - \\bar{Y}\\right)^2.\n",
    "$$\n",
    "\n",
    "Supposons que la variance $V(Y)$ existe et est finie. \n",
    "Le théorème central limit (théorème de Lindeberg–Lévy) indique que la distribution de la moyenne empirique $\\bar{Y}$ converge en loi vers la loi gaussienne. \n",
    "Plus précisément, \n",
    "$$\n",
    "\\sqrt{n} \\left(\\bar{Y} - E(Y)\\right) \\xrightarrow{d} \\mathcal{N}(0,V(Y)).\n",
    "$$\n",
    "Cela implique :\n",
    "$$\n",
    "\\bar{Y} \\xrightarrow{d} \\mathcal{N}\\left(E(Y),\\frac{V(Y)}{n}\\right).\n",
    "$$\n",
    "\n",
    "C'est pourquoi on peut dire que, de manière approchée, la précision absolue d'estimation de la moyenne dépend de $\\frac{\\sqrt{V(Y)}}{\\sqrt{n}}$. Par exemple, si $V(Y)=1$, alors il faut $n=10^4$ simulations pour obtenir une erreur absolue de l'ordre de $\\frac{1}{10^2} = 0.01$. D'un autre point de vue, pour réduire d'un facteur 2 l'erreur absolue d'estimation, il faut multiplier par 4 la taille de l'échantillon. \n",
    "\n",
    "Cette propriété permet d'obtenir un intervalle de confiance asymptotique \n",
    "approché pour l'espérance. \n",
    "Soit $\\alpha\\in[0,1/2]$ un niveau de confiance. \n",
    "Soient $q_{\\alpha/2}$ et $q_{1-\\alpha/2}$ les quantiles de niveaux $\\alpha/2$ et $1-\\alpha/2$. \n",
    "Alors, lorsque $n$ est grand :\n",
    "$$\n",
    "P\\left(\\left[\\bar{Y} - q_{\\alpha/2} \\hat{\\sigma},\\bar{Y} + q_{1-\\alpha/2} \\hat{\\sigma}\\right] \\ni E(Y)\\right) = 1 - \\alpha.\n",
    "$$\n",
    "Cet intervalle est asymptotique, car il n'est vrai que lorsque $n$ est grand. Il est approché, car nous avons remplacé l'écart-type exact $\\sigma_Y$ par l'écart-type empirique $\\hat{\\sigma}$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
