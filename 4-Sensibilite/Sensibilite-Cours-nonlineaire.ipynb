{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse de sensibilité non linéaire\n",
    "\n",
    "## Références\n",
    "\n",
    "- O. Le Maître and O. Knio, Stochastic Spectral Methods for Uncertainty Quantication with Applications to Computational Fluid Dynamics, Series on Scientific Computation, Springer, 520 pages, (2010)\n",
    "- Andrea Saltelli, Stefano Tarantola, Francesca Compolongo, and Marco Ratto. Sensitivity analysis in practice. John Wiley and Sons, 2004.\n",
    "- Jean-Marc Martinez. Gdr Ondes & Mascot Num , Analyse de sensibilité globale par décomposition de la variance. Janvier 2011.\n",
    "- M. Baudin, K. Boumhaout, T. Delage, B. Iooss and J-M. Martinez. Numerical stability of Sobol' indices estimation formula, Proceedings of the SAMO 2016 Conference, Reunion Island, France, December 2016\n",
    "- Lois asymptotiques des estimateurs des indices de Sobol. Application de la méthode delta. Rapport technique. EDF par Phiméca.\n",
    "\n",
    "Le texte présenté ci-dessous est en partie un résumé de :\n",
    "\n",
    "- Introduction to sensitivity analysis with NISP, Michael Baudin (EDF), Jean-Marc Martinez (CEA), Version 0.5, February 2014\n",
    "\n",
    "qui contient les démonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Définition intuitive\n",
    "\n",
    "Dans cette section, nous définissons les indices de Sobol' du premier ordre et totaux. \n",
    "\n",
    "Soit $X\\in\\mathbb{R}^p$ où $p$ est la dimension du vecteur d'entrée. Soit $g$ une fonction de $\\mathbb{R}^p$ vers $\\mathbb{R}$ définie par :\n",
    "\n",
    "$$\n",
    "Y = g(X)\n",
    "$$\n",
    "\n",
    "où $Y\\in\\mathbb{R}$, pour tout $X\\in\\mathbb{R}^p$. \n",
    "\n",
    "On suppose que $X$ est une variable aléatoire de loi connue. On suppose de plus, et c'est une hypothèse très limitative, que les lois marginales de $X$ sont indépendantes.\n",
    "\n",
    "Dans la suite du texte, la présentation est grandement simplifiée si l'on suppose que les lois marginales du vecteur aléatoire $X$ sont uniformes entre 0 et 1. En d'autres termes, on fait l'hypothèse que $X$ est dans le cube unité $[0,1]^p$ :\n",
    "\n",
    "$$\n",
    "Y = g(X), \\qquad X\\in[0,1]^p.\n",
    "$$\n",
    "\n",
    "Cette dernière simplification n'est pas une limitation dans l'analyse, car il est possible de démontrer des résultats presque identiques sans cette restriction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On note $E(Y)$ l'espérance de $Y$ et $V(Y)$ la variance de $Y$. L'objectif de l'analyse de sensibilité globale est de quantifier l'impact de la variabilité de chaque variable d'entrée $X_i$ sur la variabilité de $Y$. En d'autres termes, on cherche à quantifier l'impact de $X_i$ sur $V(Y)$. \n",
    "\n",
    "Pour $i\\in\\{1,...,p\\}$, supposons que $X_i$ est une variable qui a un *fort impact* sur $Y$. Essayons de donner un sens probabiliste qui permette de quantifier cet impact. Cela implique que, si on fixe $X_i$ à une valeur $x_i\\in\\mathbb{R}$ donnée, alors la variable $Y|X_i=x_i$ a une variabilité moins grande. En d'autres termes \n",
    "\n",
    "$$\n",
    "V(Y|X_i=x_i) \\ll V(Y).\n",
    "$$\n",
    "\n",
    "Par conséquent, la différence \n",
    "\n",
    "$$\n",
    "\\delta_i = V(Y) - V(Y|X_i=x_i)\n",
    "$$\n",
    "\n",
    "est grande. \n",
    "La difficulté est que la différence $\\delta_i$ dépend de la valeur de $x_i$ que nous choisissons. La valeur la plus appropriée est sans doute $x_i=E(X_i)$, mais, puisque $X_i$ est une variable aléatoire, il y a d'autres valeurs possibles. C'est pourquoi on souhaite obtenir la différence moyenne $E(\\delta_i)$ :\n",
    "\n",
    "\\begin{align*}\n",
    "E(\\delta_i) &= E\\left[V(Y) - V(Y|X_i)\\right] \\\\\n",
    "&= V(Y) - E[V(Y|X_i)].\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Le théorème de la variance totale implique :\n",
    "\n",
    "$$\n",
    "V(Y) = E[V(Y|X_i)] + V[E(Y|X_i)]\n",
    "$$\n",
    "\n",
    "pour $i=1,...,p$. \n",
    "\n",
    "On substitue $V(Y)$ dans l'équation précédente, ce qui implique :\n",
    "\n",
    "$$\n",
    "E(\\delta_i) = V[E(Y|X_i)]\n",
    "$$\n",
    "\n",
    "pour $i=1,...,p$. \n",
    "La difficulté est que l'expression précédente est absolue, et non relative à la valeur de $V(Y)$. C'est pourquoi on normalise le terme précédent par $V(Y)$. \n",
    "\n",
    "Par définition, l'indice du premier ordre de la variable $X_i$ par rapport à $g(X)$ est :\n",
    "\n",
    "$$\n",
    "S_i = \\frac{V[E(g(X)|X_i)]}{V(g(X))}\n",
    "$$\n",
    "\n",
    "pour $i=1,...,p$. \n",
    "\n",
    "L'analyse précédente montre que, si la variable $X_i$ a un impact important sur la variabilité de $Y$, alors $S_i$ est grand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Décomposition de Sobol'\n",
    "\n",
    "If $g$ can be integrated in $[0,1]^p$, then there is a unique \n",
    "decomposition \n",
    "\n",
    "$$\n",
    "y = h_0 + \\sum_{i=1,2,\\ldots,p} h_i(x_i) + \\sum_{1\\leq i < j \\leq p} h_{i,j}(x_i,x_j) + \\ldots + \n",
    "h_{1,2,\\ldots,p}(x_1,x_2,\\ldots,x_p),\n",
    "$$\n",
    "\n",
    "where $h_0$ is a constant and the functions of the decomposition satisfy the equalities \n",
    "\n",
    "$$\n",
    "\\int_0^1 h_{i_1,\\ldots,i_s}(x_{i_1},\\ldots,x_{i_s})dx_{i_k} = 0,\n",
    "$$\n",
    "\n",
    "for any $k=1,2,\\ldots,s$ and any indices $1\\leq i_1< i_2< \\ldots< i_s\\leq p$ and \n",
    "$s=1,2,\\ldots,p$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us denote by the vector $u$ the set of indices $(i_1,i_2,\\ldots,i_s)$, \n",
    "where $1\\leq i_1< i_2< \\ldots< i_s\\leq p$. \n",
    "Hence, the vector $x_u=(x_{i_1},x_{i_2},\\ldots,x_{i_s})$ is made of the \n",
    "components of the vector $x=(x_1,x_2,\\ldots,x_p)$ which indices are in the set $u$. \n",
    "The equation can then be written in the \n",
    "more compact, but more abstract, equation:\n",
    "\n",
    "$$\n",
    "g(x) = \\sum_{u\\subseteq \\{1,2,\\ldots,p\\}} h_u(x_u).\n",
    "$$\n",
    "\n",
    "Moreover, we define the function $h_\\emptyset=h_0$, so that \n",
    "the equations are consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Décomposition de la variance\n",
    "\n",
    "La variance de la fonction $g$ peut être décomposée en :\n",
    "\n",
    "$$\n",
    "V(Y)=\\sum_{i=1}^p V_i + \\sum_{1\\leq i < j\\leq p} V_{i,j} + \\ldots + V_{1,2,\\ldots,p},\n",
    "$$\n",
    "\n",
    "où \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "V_i &=& V(h_i(X_i)), \\label{eq-sde-varvi1-2} \\\\\n",
    "V_{i,j} &=& V(h_{i,j}(X_i,X_j)), \\\\\n",
    "V_{i,j,k} &=& V(h_{i,j,k}(X_i,X_j,k)), \\\\\n",
    "\\ldots&&\\\\\n",
    "V_{1,2,\\ldots,p} &=& V(h_{1,2,\\ldots,p}(X_1,X_2,\\ldots,X_p)),\n",
    "\\label{eq-sde-varv12p-2}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "or, more generally, \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "V_u = V(h_u(X_u)),\n",
    "\\end{eqnarray*}\n",
    "\n",
    "for any $u \\subseteq \\{1,2,\\ldots,p\\}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, the sensitivity indices are equal to \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "S_i &=& \\frac{V_i}{V(Y)} , \\\\\n",
    "S_{i,j} &=& \\frac{V_{i,j}}{V(Y)} , \\\\\n",
    "S_{i,j,k} &=& \\frac{V_{i,j,k}}{V(Y)} , \\\\\n",
    "\\ldots && \\\\\n",
    "S_{1,2,\\ldots,p} &=& \\frac{V_{1,2,\\ldots,p}}{V(Y)}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "pour $i=1,...,p$.\n",
    "\n",
    "On peut démontrer que l'indice du premier ordre $S_i$ présenté dans l'équation précédente est égal à l'indice défini sur la base de la variance conditionnelle.\n",
    "\n",
    "For any $i=1,2,\\ldots,p$, the total sensitivity index $T_i$ \n",
    "with respect to the variable $X_i$ is the \n",
    "sum of all the sensitivity indices associated with the variable $X_i$, i.e. \n",
    "\n",
    "$$\n",
    "T_i = \\sum_{u\\ni i} S_u.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Définition de l'indice de sensibilité total\n",
    "\n",
    "Soit $X\\in\\mathbb{R}^p$. Pour tout $i=1,...,p$, soit $X_{-i}\\in\\mathbb{R}^{p-1}$ le vecteur aléatoire constitué de toutes les composantes de $X$, sauf la i-ème. En d'autres termes, on a :\n",
    "\n",
    "$$\n",
    "X_{-i} = (X_1,...,X_{i-1},X_{i+1},...,X_p).\n",
    "$$\n",
    "\n",
    "Par conséquent, le vecteur $X$ est constitué des composantes $X_i$ et $X_{-i}$, ce qui implique $X = (X_i,X_{-i})$\n",
    "pour $i=1,...,p$.\n",
    "\n",
    "Ainsi la variable d'entrée de $Y=g(X)$ peut se décomposer en une part qui ne dépend que de $X_i$ et une autre part qui ne dépend que des composantes différentes de $X_i$ :\n",
    "\n",
    "$$\n",
    "Y = g(X_i,X_{-i}),\n",
    "$$\n",
    "\n",
    "pour $i=1,...,p$.\n",
    "Par définition, l'indice de sensibilité total de la variable d'entrée $X_i$ par rapport à la sortie $Y$ est :\n",
    "\n",
    "$$\n",
    "T_i = 1 - \\frac{V[E(g(X)|X_{-i})]}{V(g(X))}\n",
    "$$\n",
    "\n",
    "pour $i=1,...,p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyse des indices\n",
    "\n",
    "L'analyse des indices de sensibilité peut être faite en considérant leur valeur absolue et en les comparant entre eux. \n",
    "\n",
    "- On a $S_i,T_i \\in[0,1]$. \n",
    "- On a $S_i \\leq T_i$, autrement dit l'indice du premier ordre est inférieur à l'indice total. \n",
    "- L'indice du premier ordre $S_i$ représente l'impact de la variable $X_i$ seule. \n",
    "- L'indice total $T_i$ représente l'impact de la variable $X_i$, y compris ses interactions avec les autres variables. \n",
    "- Si $T_i=0$ alors la variable $X_i$ peut être remplacée par une constante. En effet, même lorsqu'elle interagit avec autres variables, elle n'a pas d'impact sur la variance de $Y$. \n",
    "- Si $S_i=T_i$ alors la variable $X_i$ n'interagit pas avec les autres variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation des indices de sensibilité de Sobol'\n",
    "\n",
    "Deux éléments s'avèrent déterminants lorsqu'on estime les indices de Sobol'.\n",
    "\n",
    "- On pourrait considérer la variable $Y$ directement pour calculer les indices de sensibilité. En fait, il est plus pertinent de centrer $Y$ en le remplaçant par $Y-E(Y)$.\n",
    "- On pourrait considérer deux boucles imbriquées pour estimer la variance de l'espérance conditionnelle. En fait, estimer les indices peut être réalisé par une intégration en dimension 2p-1, au lieu de deux intégrales imbriquées en dimensions p-1 et 1.\n",
    "\n",
    "Nous allons successivement approfondir ces deux sujets dans les deux paragraphes qui suivent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation des indices : centrage de la fonction\n",
    "\n",
    "Soit $c\\in\\mathbb{R}$ une constante. Ajouter la constante $c$ à la fonction $g$ ne change pas ses indices de sensibilité. \n",
    "En effet, soit $\\tilde{g}$ la fonction définie par :\n",
    "\n",
    "$$\n",
    "\\tilde{g}(X) = g(X) + c\n",
    "$$\n",
    "\n",
    "pour tout $X\\in[0,1]^p$. \n",
    "\n",
    "Alors l'espérance de $\\tilde{g}$ est égale à :\n",
    "\n",
    "$$\n",
    "E[\\tilde{g}(X)] = E[g(X) + c] = E[g(X)] + c.\n",
    "$$\n",
    "\n",
    "Sa variance est :\n",
    "\n",
    "\\begin{align*}\n",
    "V[\\tilde{g}(X)] \n",
    "&= E[(\\tilde{g}(X)-E(\\tilde{g}(X)))^2] \\\\\n",
    "&= E[(g(X) + c-E[g(X)] - c)^2] \\\\\n",
    "&= E[(g(X) -E[g(X)])^2] \\\\\n",
    "& = V[g(X)] \n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En d'autres termes, cela *décale* l'espérance, mais la variance est inchangée. \n",
    "De la même manière, la variance conditionnelle est inchangée, puisque :\n",
    "\n",
    "$$\n",
    "V[E(\\tilde{g}(X)|X_i)] = V[E(g(X)|X_i)]\n",
    "$$\n",
    "\n",
    "pour $i=1,...,p$.\n",
    "C'est pourquoi les indices de sensibilité du premier ordre et totaux sont inchangés lorsqu'on considère $\\tilde{g}$ en lieu et place de $g$:\n",
    "\n",
    "$$\n",
    "S_i = \\frac{V[E(\\tilde{g}(X)|X_i)]}{V(\\tilde{g}(X))}, \\qquad \n",
    "T_i = 1 - \\frac{V[E(\\tilde{g}(X)|X_{-i})]}{V(\\tilde{g}(X))}.\n",
    "$$\n",
    "\n",
    "Pour estimer les indices de sensibilité, on choisit de centrer la fonction par son espérance, c'est à dire que l'on choisit la constante $c=-E[g(X)]$. \n",
    "En d'autres termes, on considère la fonction :\n",
    "\n",
    "$$\n",
    "\\tilde{g}(X) = g(X) - E[g(X)]\n",
    "$$\n",
    "\n",
    "pour tout $X\\in[0,1]^p$. \n",
    "Par conséquent, \n",
    "\n",
    "$$\n",
    "E[\\tilde{g}(X)]=E[g(X) - E(g(X))]\n",
    "$$\n",
    "\n",
    "ce qui mène à l'équation :\n",
    "\n",
    "$$\n",
    "E[\\tilde{g}(X)]=0.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En pratique, dans OpenTURNS, on estime donc d'abord la moyenne $E[g(X)]$ et la variance $V[g(X)]$ à l'aide d'une première technique d'estimation comme, par exemple, un plan Monte-Carlo simple. \n",
    "\n",
    "Il s'avère que ce centrage permet de stabiliser l'estimation des indices de Sobol' pour des raisons à la fois numériques (l'estimation de la variance est plus précise, car elle mobilise des termes dont l'amplitude est réduite) et statistiques (l'estimation de la variance n'est plus biaisée)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Indices de sensibilité du premier ordre par une intégrale en dimension étendue\n",
    "\n",
    "Pour $i=1,...,p$, on souhaite estimer l'indice de sensibilité du premier ordre $S_i$ et l'indice total $T_i$. \n",
    "\n",
    "Supposons que l'on utiliser une méthode d'échantillonage pour estimer les intégrales requises. Il peut sembler que deux intégrations imbriquées sont nécessaires :\n",
    "\n",
    "- une première pour estimer l'espérance conditionnelle,\n",
    "- une seconde pour estimer la variance.\n",
    "\n",
    "Si chaque boucle nécessite $n$ calculs, alors le coût de calcul total est $n^2$, ce qui est *formidable*. \n",
    "\n",
    "En fait, une astuce proposée par Sobol' permet de remplacer cette double intégration par une seule intégration en dimension $2p-1$. En effet, le numérateur de l'indice de Sobol' du premier ordre est :\n",
    "\n",
    "$$\n",
    "V [E(Y|X_i)] \n",
    "= E\\left[ E(Y|X_i)^2 \\right] - E(Y)^2. \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En pratique, OpenTURNS utilise la fonction $\\tilde{g}$ à la place de $g$, de telle sorte que $E[\\tilde{g}(X)]=0$. Non seulement cela simplifie l'expression précédente sur le plan formel, mais, surtout, cela évite de former une différence de deux carrés, pouvant mener à une perte de précision \"catastrophique\" par *cancellation*.\n",
    "\n",
    "Pour tout $i=1,...,p$, soit $x_i\\in[0,1]$. \n",
    "Par définition de l'espérance conditionnelle, \n",
    "\n",
    "$$\n",
    "E(Y|X_i=x_i) = \\int_{[0,1]^{p-1}} g(x_i,x_{-i}) dx_{-i}.\n",
    "$$\n",
    "\n",
    "Soit $Z\\in[0,1]^p$ un vecteur aléatoire de même loi que $X$ et indépendant de $X$. Alors la densité de probabilité du vecteur aléatoire $(X,Z)$ est le produit des densités marginales :\n",
    "\n",
    "$$\n",
    "f_{X,Z}(x,z) = f_X(x) f_Z(z)\n",
    "$$\n",
    "\n",
    "pour tout $x,z\\in[0,1]^p$. \n",
    "\n",
    "Soit $Z_{-i}$ le vecteur aléatoire de dimension réduite : $Z_{-i}\\in[0,1]^{p-1}$ :\n",
    "\n",
    "$$\n",
    "Z_{-i} = (Z_1,...,Z_{i-1},Z_{i+1},...,Z_p).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On peut démontrer que : \n",
    "\n",
    "$$\n",
    "E[E(Y|X_i)^2]\n",
    "= \\int_{[0,1]^{2p-1}} g(x_i,x_{-i}) g(x_i,z_{-i}) dx_{-i} dz_{-i} dx_i.  \n",
    "$$\n",
    "\n",
    "Soit $x^e\\in[0,1]^{2p-1}$ la variable aléatoire en dimension $2p-1$ suivante :\n",
    "\n",
    "\\begin{align*}\n",
    "x^{e} & = \n",
    "(x_1, x_2, ..., x_p, z_1, ..., z_{i-1}, z_{i+1}, ..., z_{p})^T \\\\\n",
    "&= (x_i,x_{-i},z_{-i}).\n",
    "\\end{align*}\n",
    "\n",
    "Soit $g^{e}$ la fonction en dimension étendue définie par :\n",
    "\n",
    "\\begin{align*}\n",
    "g^{e}\\left(x^e\\right) \n",
    "&= g(x_1, x_2, ..., x_i, ..., x_p) g(z_1, ..., z_{i-1}, x_i, z_{i+1}, ..., z_{p}) \\\\\n",
    "&= g(x_i,x_{-i}) g(x_i,z_{-i})\n",
    "\\end{align*}\n",
    "\n",
    "pour tout $x\\in[0,1]^p$ et tout $z_{-i}\\in[0,1]^{p-1}$. \n",
    "\n",
    "L'équation précédente montre que le numérateur de l'indice de Sobol' du premier ordre est égal à l'intégrale de la fonction $g^{e}$ :\n",
    "\n",
    "$$\n",
    "E[E(Y|X_i)^2]\n",
    "= \\int_{[0,1]^{2p-1}} g^e(x^e) dx^e.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Indices de sensibilité total par une intégrale en dimension étendue\n",
    "\n",
    "Pour estimer l'indice de sensibilité total, nous devons estimer :\n",
    "\n",
    "$$\n",
    "V[E(g(X)|X_{-i})] = E[E(g(X)|X_{-i})^2] - E(Y)^2.\n",
    "$$\n",
    "\n",
    "On peut démontrer que : \n",
    "\n",
    "$$\n",
    "E[E(g(X)|X_{-i})^2]\n",
    "= \\int_{[0,1]^{p+1}} g(x_i,x_{-i}) g(z_i,x_{-i}) dx_{-i} dx_i dz_i.  \n",
    "$$\n",
    "\n",
    "Soit $x^f\\in[0,1]^{p+1}$ la variable aléatoire en dimension $p+1$ suivante :\n",
    "\n",
    "\\begin{align*}\n",
    "x^{f} & = \n",
    "(x_1, x_2, ..., x_p, z_{i})^T \\\\\n",
    "&= (x_i,x_{-i},z_{i}).\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Soit $g^{f}$ la fonction en dimension étendue définie par :\n",
    "\n",
    "\\begin{align*}\n",
    "g^{f}\\left(x^f\\right) \n",
    "&= g(x_1, x_2, ..., x_i, ..., x_p) g(x_1, ..., x_{i-1}, z_i, x_{i+1}, ..., x_{p}) \\\\\n",
    "&= g(x_i,x_{-i}) g(z_i,x_{-i})\n",
    "\\end{align*}\n",
    "\n",
    "pour tout $x\\in[0,1]^p$ et tout $z_{i}\\in[0,1]$. \n",
    "\n",
    "Le numérateur de l'indice de Sobol' total est égal à l'intégrale de la fonction $g^{f}$ :\n",
    "\n",
    "$$\n",
    "E[E(Y|X_{-i})^2]\n",
    "= \\int_{[0,1]^{p+1}} g^f\\left(x^f\\right) dx^f.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimateur des indices de sensibilité\n",
    "\n",
    "Dans cette section, nous présentons une modification de la méthode de Sobol'-Saltelli pour l'estimation des indices du premier ordre. Cette méthode est une variante de la méthode dite \"pick-freeze\". Pour l'estimation des indices du premier ordre, la méthode de Saltelli est fondée sur un échantillonnage Monte-Carlo simple pour l'estimation de l'intégrale en dimension $2p-1$ associée à la fonction $g^e$. Pour les indices totaux, la méthode de Saltelli est fondée sur une intégrale en dimension $p+1$ associée à la fonction $g^f$. \n",
    "\n",
    "D'une manière plus générale, toute méthode d'estimation d'une intégrale est éligible pour estimer les indices de Sobol' comme par exemple :\n",
    "\n",
    "- une séquence à faible discrépance,\n",
    "- un plan Latin Hypercube Sampling (LHS).\n",
    "\n",
    "Pour les deux derniers types de plan d'expériences, l'estimation des indices du premier ordre impose de constituer un plan d'expériences en dimension $2p-1$ (une méthode fondée sur la concaténation de deux plans en dimension $p$ et $p-1$ ne peut pas être convergente).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L'algorithme est le suivant.\n",
    "\n",
    "- On considère un plan d'expériences de type Monte-Carlo simple de taille $n$ et de dimension $2p-1$ pour la variable $x^e$, constitué des points $x^{e(1)}, ..., x^{e(n)}$. Les $p$ premières colonnes de ce plan d'expériences constituent un plan de type Monte-Carlo simple de taille $n$ et de dimension $p$ pour la variable $X$, constitué des points $x^{(1)}, ..., x^{(n)}$. \n",
    "- On estime la moyenne $E(Y)$ par la moyenne empirique :\n",
    "\n",
    "$$\n",
    "\\bar{Y} = \\frac{1}{n} \\sum_{j=1}^n g\\left(x^{(j)}\\right).\n",
    "$$\n",
    "\n",
    "- On considère la fonction centrée $\\tilde{g}$ définie par :\n",
    "\n",
    "$$\n",
    "\\tilde{g}(x) = g(x) - \\bar{Y},\n",
    "$$\n",
    "\n",
    "pour tout $x\\in[0,1]^p$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- On estime la variance $V(Y)$ par la variance empirique *biaisée* :\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{j=1}^n \\tilde{g}\\left(x^{(j)}\\right)^2.\n",
    "$$\n",
    "\n",
    "- Pour $i=1,...,p$, on estime $E[E(Y|X_i)^2]$ par la moyenne empirique :\n",
    "\n",
    "$$\n",
    "\\hat{E}_i = \\frac{1}{n} \\sum_{j=1}^n \\tilde{g}^e\\left(x^{e(j)}\\right)\n",
    "$$\n",
    "\n",
    "où $\\tilde{g}^e$ est la fonction centrée en dimension étendue $2p-1$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* L'indice de sensibilité du premier ordre est estimé par :\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{S}_i &= \\frac{\\hat{E}_i}{\\hat{\\sigma}^2} \\\\\n",
    "&= \\frac{\\frac{1}{n} \\sum_{j=1}^n \\tilde{g}^e\\left(x^{e(j)}\\right)}{\\frac{1}{n} \\sum_{j=1}^n \\tilde{g}\\left(x^{(j)}\\right)^2} \\\\\n",
    "&= \\frac{\\frac{1}{n} \\sum_{j=1}^n \\tilde{g}\\left(x_i^{(j)},x_{-i}^{(j)}\\right) \\tilde{g}\\left(x_i^{(j)},z_{-i}^{(j)}\\right)}{\\frac{1}{n} \\sum_{j=1}^n \\tilde{g}\\left(x^{(j)}\\right)^2}\n",
    "\\end{align*}\n",
    "\n",
    "où $\\tilde{g}$ est la fonction centrée.\n",
    "\n",
    "- On estime $E[E(Y|X_{-i})^2]$ par la moyenne empirique :\n",
    "\n",
    "$$\n",
    "\\hat{E}_{-i} = \\frac{1}{n} \\sum_{j=1}^n \\tilde{g}^f\\left(x^{f(j)}\\right).\n",
    "$$\n",
    "\n",
    "où $\\tilde{g}^f$ est la fonction centrée en dimension étendue $p+1$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* L'indice de sensibilité total est estimé par :\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{T}_i &= 1 - \\frac{\\hat{E}_{-i}}{\\hat{\\sigma}^2} \\\\\n",
    "&= 1 - \\frac{\\frac{1}{n} \\sum_{j=1}^n \\tilde{g}^f\\left(x^{f(j)}\\right)}{\\frac{1}{n} \\sum_{j=1}^n \\tilde{g}\\left(x^{(j)}\\right)^2} \\\\\n",
    "&= 1 - \\frac{\\frac{1}{n} \\sum_{j=1}^n \\tilde{g}\\left(x_i^{(j)},x_{-i}^{(j)}\\right) \\tilde{g}\\left(z_{i}^{(j)},x_{-i}^{(j)}\\right)}{\\frac{1}{n} \\sum_{j=1}^n \\tilde{g}\\left(x^{(j)}\\right)^2}\n",
    "\\end{align*}\n",
    "\n",
    "où $\\tilde{g}$ est la fonction centrée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La méthode pick-freeze centrée pour l'estimation des indices de sensibilité\n",
    "\n",
    "Dans cette section, nous reformulons l'estimation des indices de sensibilité à l'aide de la méthode dit *\"pick-freeze\"*. Cette méthode clarifie les plans d'expériences impliqués dans l'estimation des indices de sensibilité du premier ordre et totaux, en particulier les variables différentes et communes dans chaque plan d'expériences fondé sur la méthode de Monte-Carlo simple.\n",
    "\n",
    "On considère trois plans d'expériences générés par échantillonnage Monte-Carlo simple notés $A$ et $B$ de taille $n$ et de dimension $p$. Plus précisément, on considère \n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "a_{1,1} & a_{1,2} & \\cdots & a_{1,p} \\\\\n",
    "a_{2,1} & a_{2,2} & \\cdots & a_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{n,1} & a_{n,2} & \\cdots & a_{n,p}\n",
    "\\end{pmatrix}\n",
    ", \\qquad \n",
    "B=\n",
    "\\begin{pmatrix}\n",
    "b_{1,1}  & b_{1,2}  & \\cdots  & b_{1,p}  \\\\\n",
    "b_{2,1}  & b_{2,2}  & \\cdots  & b_{2,p}  \\\\\n",
    "\\vdots  & \\vdots  & \\ddots  & \\vdots  \\\\\n",
    "b_{n,1}  & b_{n,2}  & \\cdots  & b_{n,p} \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pour $i=1,...,p$, on souhaite estimer l'indice du premier ordre $S_i$. Pour cela on créée le plan $E$ sur la base des deux plans d'expériences $A$ et $B$. Ce plan est créé à partir de $A$, dans lequel la ième colonne est celle de $B$. Plus précisément, le plan $E$ est le suivant :\n",
    "\n",
    "$$\n",
    "E=\n",
    "\\begin{pmatrix}\n",
    "a_{1,1}  & a_{1,2}  & \\cdots  & b_{1,i} & \\cdots  & a_{1,p}  \\\\\n",
    "a_{2,1}  & a_{2,2}  & \\cdots  & b_{2,i} & \\cdots  & a_{2,p}  \\\\\n",
    "\\vdots  & \\vdots  &   & \\vdots &   & \\vdots  \\\\\n",
    "a_{n,1}  & a_{n,2}  & \\cdots  & b_{n,i} & \\cdots  & a_{n,p} \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On note $Y^{A,c}$, $Y^{B,c}$ et $Y^{E,c}$ le vecteur des sorties centrées du modèle $g$ sur les plans $A$, $B$ et $E$ :\n",
    "\n",
    "$$\n",
    "Y^{A,c} = g(A) - \\bar{Y}^A, \\qquad \n",
    "Y^{B,c} = g(B) - \\bar{Y}^B, \\qquad \n",
    "Y^{E,c} = g(E) - \\bar{Y}^E,\n",
    "$$\n",
    "\n",
    "où $\\bar{Y}^A$, $\\bar{Y}^B$ et $\\bar{Y}^E$ sont respectivement la moyenne empirique des \n",
    "sorties des plans $A$, $B$ et $E$ :\n",
    "\n",
    "$$\n",
    "\\bar{Y}^A = \\frac{1}{n} \\sum_{j=1}^n Y^{A,j}, \\qquad\n",
    "\\bar{Y}^B = \\frac{1}{n} \\sum_{j=1}^n Y^{B,j}, \\qquad\n",
    "\\bar{Y}^E = \\frac{1}{n} \\sum_{j=1}^n Y^{E,j}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pour $i=1,...,p$, l'indice de sensibilité du premier ordre peut être estimé par :\n",
    "\n",
    "$$\n",
    "S_i = \\frac{\\sum_{j=1}^n Y^{c,B,j} Y^{c,E,j}}{\\sum_{j=1}^n \\left(Y^{c,A,j}\\right)^2}.\n",
    "$$\n",
    "\n",
    "Pour $i=1,...,p$, l'indice de sensibilité total peut être estimé par :\n",
    "\n",
    "$$\n",
    "T_i = 1 - \\frac{\\sum_{j=1}^n Y^{c,A,j} Y^{c,E,j}}{\\sum_{j=1}^n \\left(Y^{c,A,j}\\right)^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La méthode de Martinez pour l'estimation des indices de Sobol'\n",
    "\n",
    "La méthode de J.-M.Martinez est certainement l'estimateur le plus simple pour les indices de Sobol'. \n",
    "\n",
    "Cet estimateur repose sur le coefficient de corrélation linéaire empirique $\\rho_n$ de Pearson. \n",
    "\n",
    "Soient $X$ et $Y$ deux vecteurs de dimension $n$. Alors\n",
    "\n",
    "$$\n",
    "\\rho_n(X,Y) = \\frac{\\sum_{j=1}^n (x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{j=1}^n (x_i - \\bar{x})^2(y_i-\\bar{y})^2}},\n",
    "$$\n",
    "\n",
    "où $\\bar{x}$ et $\\bar{y}$ sont les moyennes empiriques des vecteurs $X$ et $Y$ :\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\sum_{j=1}^n x_i, \\qquad \\bar{y} = \\sum_{j=1}^n y_i.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sur la base des notations précédentes, l'estimateur de Martinez est :\n",
    "\n",
    "$$\n",
    "S_i = \\rho_n\\left(Y^B,Y^E\\right), \\qquad T_i = 1 - \\rho_n\\left(Y^A,Y^E\\right).\n",
    "$$\n",
    "\n",
    "On remarque que, puisque le coefficient de corrélation linéaire est centré, l'estimateur de Martinez évite par construction les difficultés associés aux autres estimateurs. \n",
    "\n",
    "De plus, si la sortie $Y$ suit la loi gaussienne, alors J.-M.Martinez a utilisé la transformation de Fisher pour obtenir la loi de l'estimateur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Notre expérience est que, si la fonction est centrée, les différents estimateurs d'OpenTURNS (Saltelli, Jansen, Mauntz-Kucherenko et Martinez) sont souvents équivalents du point de vue de la précision, tant sur les indices du premier ordre que les indices totaux. De plus, leur comportement est également comparablement imparfait lorsqu'il s'agit d'estimer des indices proches de zéro : en effet, estimer une moyenne proche de zéro par une méthode de Monte-Carlo est difficile avec une grande précision *relative*. Puisque les indices sont entre zéro et un, dans la plupart des situations, une estimation avec un à deux chiffres significatifs est suffisante. \n",
    "\n",
    "De plus, la méthode delta peut être utilisée pour obtenir la distribution asymptotique gaussienne des estimateurs."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
