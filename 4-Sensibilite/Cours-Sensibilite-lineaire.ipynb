{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analyse de sensibilité linéaire\n",
    "\n",
    "## Analyse du modèle linéaire\n",
    "\n",
    "Assume that the random variables $X_i$ are *independent*, with \n",
    "mean $E(X_i)$ and finite variances $V(X_i)$, for $i=1,2,\\ldots,p$.\n",
    "Let us suppose that $Y$ is an affine function of the\n",
    "variables $X_i$:\n",
    "$$\n",
    "Y = g(X) = \\beta_0 + \\sum_{i=1,2,\\ldots,p} \\beta_i X_i,\n",
    "$$\n",
    "where $\\beta_i$ are real parameters, for $i=1,2,\\ldots,p$.\n",
    "\n",
    "The expectation of the sum of variables is the sum of the expectations, \n",
    "so that \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[Y] \n",
    "&=& \\mathbb{E}[\\beta_0] + \\sum_{i=1,2,\\ldots,p} \\mathbb{E}[\\beta_i X_i] \\\\\n",
    "&=& \\beta_0 + \\sum_{i=1,2,\\ldots,p} \\beta_i \\mathbb{E}[X_i].\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that the previous computation can be performed \n",
    "even when the variables are dependent. \n",
    "As we are going to see, we can derive a similar equality for the \n",
    "variance, although the independence of the variable is \n",
    "then a strict requirement. \n",
    "\n",
    "The importance factor of a linear model is : \n",
    "\n",
    "$$\n",
    "\\eta_i = \\frac{\\beta_i^2 V(X_i)}{V(Y)},\n",
    "$$\n",
    "\n",
    "for $i=1,2,\\ldots,p$.\n",
    "\n",
    "The standardized regression coefficient is \n",
    "\n",
    "$$\n",
    "SRC_i = \\frac{\\beta_i \\sqrt{V(X_i)}}{\\sqrt{V(Y)}},\n",
    "$$\n",
    "\n",
    "for $i=1,2,\\ldots,p$.\n",
    "\n",
    "The importance factors are the squared SRC. \n",
    "\n",
    "Obviously, we have $eta_i\\geq 0$, for $i=1,...,p$. \n",
    "Moreover, the following proposition shows that the sum of \n",
    "importance factors is equal to one.\n",
    "\n",
    "For an affine model $g$, the sum of the importance factors is one:\n",
    "\n",
    "$$\n",
    "\\eta_1 + \\eta_2 + \\ldots + \\eta_p = 1. \\qquad \\textrm{(1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Proof*\n",
    "\n",
    "Since the variables $X_i$ are independent, the variance of the \n",
    "sum of variables is the sum of the variances. \n",
    "Hence, \n",
    "$$\n",
    "\\operatorname{Var}(Y) = \\operatorname{Var}(\\beta_0) + \\sum_{i=1,2,\\ldots,p} \\operatorname{Var}(\\beta_i X_i).\n",
    "$$\n",
    "But $\\operatorname{Var}(\\beta_0)=0$ and, for each $i=1,2,\\ldots,p$, \n",
    "we have $\\operatorname{Var}(\\beta_i X_i)=\\beta_i^2 V(X_i)$. \n",
    "This leads to the equality\n",
    "$$\n",
    "\\operatorname{Var}(Y) = \\sum_{i=1,2,\\ldots,p} \\beta_i^2 \\operatorname{Var}(X_i).\n",
    "$$\n",
    "Hence, each term $\\beta_i^2 V(X_i)$ is the part of the \n",
    "total variance $\\operatorname{Var}(Y)$ which is caused by the variable $X_i$.\n",
    "We divide the previous equality by $\\operatorname{Var}(Y)$ and get the \n",
    "equation (1), which concludes the proof. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear correlation coefficient and importance factors\n",
    "\n",
    "In this section, we present the link between the linear \n",
    "correlation coefficients of an affine model, and the \n",
    "importance factors.\n",
    "\n",
    "Assume that the random variables $X_i$ are independent, with \n",
    "mean $E(X_i)$ and finite variances $V(X_i)$, for $i=1,2,\\ldots,p$.\n",
    "Let us consider the random variable $Y$, which depends linearily on the \n",
    "variables $X_i$. \n",
    "We are interested in the Pearson correlation coefficient $\\rho(Y,X_i)$.\n",
    "\n",
    "For any $i=1,2,\\ldots,p$, let us consider two jointly distributed random variables $X_i$ and $Y$. \n",
    "The covariance is \n",
    "$$\n",
    "\\operatorname{Cov}(Y, X_i) = \\mathbb{E}[(Y - \\mathbb{E}[Y])(X_i - \\mathbb{E}[X_i])],\n",
    "$$\n",
    "for $i=1,2,\\ldots,p$.\n",
    "The Pearson correlation coefficient is \n",
    "$$\n",
    "\\rho(Y,X_i) = \\frac{\\operatorname{Cov}(Y, X_i)}{\\sqrt{\\operatorname{Var}(Y)}\\sqrt{\\operatorname{Var}(X_i)}}\n",
    "$$\n",
    "for $i=1,2,\\ldots,p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assume that the output $Y$ is the affine model.\n",
    "Assume that the input variables $X_i$ are independent. \n",
    "Therefore \n",
    "$$\n",
    "SRC_i = \\rho(Y, X_i),\n",
    "$$\n",
    "for $i=1,2,\\ldots,p$.\n",
    "\n",
    "*Proof*\n",
    "\n",
    "We have \n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(Y,X_i) \n",
    "= \\operatorname{Cov}(\\beta_0,X_i) + \\beta_1 \\operatorname{Cov}(X_1,X_i)+ \\ldots + \\beta_p \\operatorname{Cov}(X_p,X_i),\n",
    "$$\n",
    "\n",
    "because the covariance function is linear with respect to \n",
    "its arguments. \n",
    "Obviously, we have $\\operatorname{Cov}(\\beta_0,X_i)=0$ since $\\beta_0$ is a constant. \n",
    "Moreover, the random variables $X_i$ are independent, which implies that $\\operatorname{Cov}(X_j,X_i) = 0$, \n",
    "for any $j \\neq i$. \n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "\\operatorname{Cov}(Y,X_i) \n",
    "= \\beta_i \\operatorname{Cov}(X_i,X_i) \n",
    "= \\beta_i V(X_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hence, the correlation coefficient can be simplified into\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Corr}(Y,X_i) \n",
    "&=& \\frac{\\beta_i \\operatorname{Var}(X_i)}{\\sqrt{V(Y)} \\sqrt{\\operatorname{Var}(X_i)}} \\\\\n",
    "&=& \\frac{\\beta_i \\sqrt{\\operatorname{Var}(X_i)}}{\\sqrt{\\operatorname{Var}(Y)}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "we recognize the SRC coefficient, \n",
    "which concludes the proof. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Régression linéaire\n",
    "\n",
    "En général, on ne sait pas si la fonction $g$ est linéaire. Dans les méthodes que nous décrivons, la fonction $g$ est une boîte noire dans laquelle la seule information observable est la sortie en fonction de l'entrée. Dans ce cas, on peut créer un modèle de regression linéaire comme une approximation de la fonction $g$. Cela permet ensuite d'utiliser les facteurs d'importance, si le modèle linéaire est une bonne approximation de la fonction $g$. Nous allons voir que cette qualité peut être quantifiée grâce au coefficient $R^2$. \n",
    "\n",
    "La prédiction du modèle linéaire est une combinaison linéaire des composantes du vecteur $\\boldsymbol{X}$ :\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\boldsymbol{X}^T (\\beta_1,...,\\beta_p)^T + \\epsilon\n",
    "$$\n",
    "\n",
    "où $\\epsilon\\in\\mathbb{R}$ est une variable aléatoire, $\\beta_0 \\in \\mathbb{R}$ et $(\\beta_1,...,\\beta_p)^T\\in\\mathbb{R}^{p}$ est le vecteur des paramètres. \n",
    "\n",
    "On peut aussi écrire le problème avec les vecteurs étendus \n",
    "\n",
    "$$\n",
    "y = (1,\\boldsymbol{X}^T) \\boldsymbol{\\beta} + \\epsilon\n",
    "$$\n",
    "\n",
    "où $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,...,\\beta_p)^T\\in\\mathbb{R}^{p+1}$ est le vecteur des paramètres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Soit $n$ la taille de l'échantillon et soit $\\{X^{(1)},...,X^{(n)}\\}$ un échantillon i.i.d. du vecteur aléatoire $\\boldsymbol{X}$. La matrice de conception du modèle linéaire est :\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "1 & X_1^{(1)} & ... & X_p^{(1)} \\\\\n",
    "1 & X_1^{(2)} & ... & X_p^{(2)} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & X_1^{(n)} & ... & X_p^{(n)}\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Soit $\\boldsymbol{y}$ le vecteur des sorties de la fonction $g$ :\n",
    "$$\n",
    "y^{(j)} = g\\left(\\boldsymbol{X}^{(j)}\\right), \\quad j=1,...,n.\n",
    "$$\n",
    "Le problème de regression linéaire consiste à résoudre le problème :\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\beta}\\in\\mathbb{R}^{p+1}} \\|\\boldsymbol{y} - A\\boldsymbol{\\beta}\\|_2.\n",
    "$$\n",
    "Si la matrice $A$ est de rang plein, la solution est unique. C'est celle donnée par les équations normales :\n",
    "$$\n",
    "\\widehat{\\boldsymbol{\\beta}} = \\left(A^T A\\right)^{-1} A^T \\boldsymbol{y}.\n",
    "$$\n",
    "En pratique, bien que la méthode des équations normales soit appropriée dans certaines circonstances, on utilise le plus souvent une méthode fondée sur une décomposition orthogonale de la matrice $A$, comme par exemple la décomposition QR ou la décomposition SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Qualité de la regression linéaire\n",
    "\n",
    "Une fois que les coefficients $\\beta$ sont calculés, on doit déterminer si le modèle linéaire est une approximation appropriée de la fonction $g$. \n",
    "Soit \n",
    "$$\n",
    "\\overline{y} = \\frac{1}{n} \\sum_{j=1}^n y^{(j)}\n",
    "$$\n",
    "la moyenne empirique des sorties $y$. \n",
    "Soit $\\hat{\\boldsymbol{y}}$ le vecteur des prédictions du modèle de regression linéaire :\n",
    "$$\n",
    "\\widehat{\\boldsymbol{y}} = A\\hat{\\boldsymbol{\\beta}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Le coefficient $R^2\\in[0,1]$ est :\n",
    "$$\n",
    "R^2 = 1- \\frac{\\sum_{j=1}^n \\left(y^{(j)} - \\hat{y}^{(j)}\\right)^2}{\\sum_{j=1}^n \\left(y^{(j)} - \\bar{y}\\right)^2}\n",
    "$$\n",
    "Le coefficient $R^2$ mesure la part de variance expliquée par le modèle linéaire. \n",
    "\n",
    "On considère souvent qu'un coefficient de prédictivité $R^2>0.9$ est le signe d'une qualité suffisante. Un coefficient $R^2<0.5$ est inacceptable pour une utilisation pratique : c'est le signe que, vraisemblablement, le modèle n'est *pas* linéaire."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
