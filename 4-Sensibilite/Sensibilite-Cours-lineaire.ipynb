{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de sensibilité linéaire\n",
    "\n",
    "## Analyse du modèle linéaire\n",
    "\n",
    "Assume that the random variables $X_i$ are independent, with \n",
    "mean $E(X_i)$ and finite variances $V(X_i)$, for $i=1,2,\\ldots,p$.\n",
    "Let us suppose that $Y$ is an affine function of the\n",
    "variables $X_i$:\n",
    "$$\n",
    "Y = g(X) = \\beta_0 + \\sum_{i=1,2,\\ldots,p} \\beta_i X_i,\n",
    "$$\n",
    "where $\\beta_i$ are real parameters, for $i=1,2,\\ldots,p$.\n",
    "\n",
    "The expectation of the sum of variables is the sum of the expectations, \n",
    "so that \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E(Y) \n",
    "&=& E(\\beta_0) + \\sum_{i=1,2,\\ldots,p} E(\\beta_i X_i) \\\\\n",
    "&=& \\beta_0 + \\sum_{i=1,2,\\ldots,p} \\beta_i E(X_i).\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Notice that the previous computation can be performed \n",
    "even when the variables are dependent. \n",
    "As we are going to see, we can derive a similar equality for the \n",
    "variance, although the independence of the variable is \n",
    "then a strict requirement. \n",
    "\n",
    "The standardized regression coefficient is \n",
    "$$\n",
    "SRC_i = \\frac{\\beta_i^2 V(X_i)}{V(Y)},\n",
    "$$\n",
    "for $i=1,2,\\ldots,p$.\n",
    "\n",
    "Obviously, we have $SRC_i\\geq 0$, for $i=1,...,p$. \n",
    "Moreover, the following proposition shows that the sum of \n",
    "standardized regression coefficients is equal to one.\n",
    "\n",
    "For an affine model $g$, the sum of the standardized regression coefficients is one:\n",
    "$$\n",
    "SRC_1 + SRC_2 + \\ldots + SRC_p = 1. \\qquad \\textrm{(1)}\n",
    "$$\n",
    "\n",
    "*Proof*\n",
    "\n",
    "Since the variables $X_i$ are independent, the variance of the \n",
    "sum of variables is the sum of the variances. \n",
    "Hence, \n",
    "$$\n",
    "V(Y) = V(\\beta_0) + \\sum_{i=1,2,\\ldots,p} V(\\beta_i X_i).\n",
    "$$\n",
    "But $V(\\beta_0)=0$ and, for each $i=1,2,\\ldots,p$, \n",
    "we have $V(\\beta_i X_i)=\\beta_i^2 V(X_i)$. \n",
    "This leads to the equality\n",
    "$$\n",
    "V(Y) = \\sum_{i=1,2,\\ldots,p} \\beta_i^2 V(X_i).\n",
    "$$\n",
    "Hence, each term $\\beta_i^2 V(X_i)$ is the part of the \n",
    "total variance $V(Y)$ which is caused by the variable $X_i$.\n",
    "We divide the previous equality by $V(Y)$ and get the \n",
    "equation (1), which concludes the proof. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear correlation coefficient and SRC indices\n",
    "\n",
    "In this section, we present the link between the linear \n",
    "correlation coefficients of an affine model, and the \n",
    "standardized regression coefficients.\n",
    "\n",
    "Assume that the random variables $X_i$ are independent, with \n",
    "mean $E(X_i)$ and finite variances $V(X_i)$, for $i=1,2,\\ldots,p$.\n",
    "Let us consider the random variable $Y$, which depends linearily on the \n",
    "variables $X_i$. \n",
    "We are interested in the correlation coefficient $Corr(Y,X_i)$.\n",
    "\n",
    "Let us consider two jointly distributed random variables $X$ and $Y$. \n",
    "The covariance is \n",
    "$$\n",
    "Cov(Y,X_i) = E[(Y-E(Y))(X_i-E(X_i))],\n",
    "$$\n",
    "for $i=1,2,\\ldots,p$.\n",
    "The linear correlation coefficient is \n",
    "$$\n",
    "Corr(Y,X_i) = \\frac{Cov(Y,X_i)}{\\sqrt{V(Y)}\\sqrt{V(X_i)}}\n",
    "$$\n",
    "for $i=1,2,\\ldots,p$.\n",
    "\n",
    "Assume that the output $Y$ is the affine model.\n",
    "Assume that the input variables $X_i$ are independent. \n",
    "Therefore \n",
    "$$\n",
    "SRC_i = Corr(Y,X_i)^2,\n",
    "$$\n",
    "for $i=1,2,\\ldots,p$.\n",
    "\n",
    "*Proof*\n",
    "\n",
    "We have \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Cov(Y,X_i) \n",
    "&=& Cov(\\beta_0,X_i) + \\beta_1 Cov(X_1,X_i)+ \\beta_2 Cov(X_2,X_i) + \\ldots  \\\\\n",
    "&& + \\beta_i Cov(X_i,X_i) + \\ldots + \\beta_p Cov(X_p,X_i),\n",
    "\\end{eqnarray*}\n",
    "\n",
    "because the covariance function is linear with respect to \n",
    "its arguments. \n",
    "Obviously, we have $Cov(\\beta_0,X_i)=0$ since $\\beta_0$ is a constant. \n",
    "Moreover, the random variables $X_i$ are independent, which implies that $Cov(X_j,X_i) = 0$, \n",
    "for any $j \\neq i$. \n",
    "Therefore, \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Cov(Y,X_i) \n",
    "&=& \\beta_i Cov(X_i,X_i) \\\\\n",
    "&=& \\beta_i V(X_i).\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Hence, the correlation coefficient can be simplified into\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Corr(Y,X_i) \n",
    "&=& \\frac{\\beta_i V(X_i)}{\\sqrt{V(Y)} \\sqrt{V(X_i)}} \\\\\n",
    "&=& \\frac{\\beta_i \\sqrt{V(X_i)}}{\\sqrt{V(Y)}}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We square the previous equality and get\n",
    "$$\n",
    "Corr(Y,X_i)^2= \\frac{\\beta_i^2 V(X_i)}{V(Y)}.\n",
    "$$\n",
    "In the previous equality, we recognize the SRC coefficient, \n",
    "which concludes the proof. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression linéaire\n",
    "\n",
    "En général, on ne sait pas si la fonction $g$ est linéaire. Dans les méthodes que nous décrivons, la fonction $g$ est une boîte noire dans laquelle la seule information observable est la sortie en fonction de l'entrée. Dans ce cas, on peut créer un modèle de regression linéaire comme une approximation de la fonction $g$. Cela permet ensuite d'utiliser les indices SRC, si le modèle linéaire est de qualité. Nous allons voir que cette qualité peut être quantifiée grâce au coefficient $R^2$. \n",
    "\n",
    "Le vecteur des prédictions du modèle linéaire est une combinaison linéaire des composantes du vecteur $X$ :\n",
    "$$\n",
    "y = \\beta_0 + X^T \\beta + \\epsilon\n",
    "$$\n",
    "où $\\epsilon$ est une variable aléatoire et $(\\beta_0,\\beta_1,...,\\beta_p)^T\\in\\mathbb{R}^{p+1}$ est le vecteur des paramètres. \n",
    "Soit $n$ la taille de l'échantillon et soit $X^{(1)},...,X^{(n)}$ un échantillon i.i.d. du vecteur aléatoire $X$. La matrice de conception du modèle linéaire est :\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "1 & X_1^{(1)} & ... & X_p^{(1)} \\\\\n",
    "1 & X_1^{(2)} & ... & X_p^{(2)} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & X_1^{(n)} & ... & X_p^{(n)}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "Soit $y$ le vecteur des sorties de la fonction $g$ :\n",
    "$$\n",
    "y^{(j)} = g\\left(X^{(j)}\\right), \\quad j=1,...,n.\n",
    "$$\n",
    "Le problème de regression linéaire consiste à résoudre le problème :\n",
    "$$\n",
    "\\min_{\\beta\\in\\mathbb{R}^p} \\|y - A\\beta\\|_2.\n",
    "$$\n",
    "Si la matrice $A$ est de rang plein, la solution est unique. C'est celle donnée par les équations normales :\n",
    "$$\n",
    "\\hat{\\beta} = \\left(A^T A\\right)^{-1} A^T y.\n",
    "$$\n",
    "En pratique, bien que la méthode des équations normales soit appropriée dans certaines circonstances, on utilise le plus souvent une méthode fondée sur une décomposition orthogonale de la matrice $A$, comme par exemple la décomposition QR ou la décomposition SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualité de la regression linéaire\n",
    "\n",
    "Une fois que les coefficients $\\beta$ sont calculés, on doit déterminer si le modèle linéaire est une approximation appropriée de la fonction $g$. \n",
    "Soit \n",
    "$$\n",
    "\\bar{y} = \\frac{1}{n} \\sum_{j=1}^n y^{(j)}\n",
    "$$\n",
    "la moyenne empirique des sorties $y$. \n",
    "Soit $\\hat{y}$ le vecteur des prédictions du modèle de regression linéaire :\n",
    "$$\n",
    "\\hat{y} = A\\hat{\\beta}.\n",
    "$$\n",
    "Le coefficient $R^2\\in[0,1]$ est :\n",
    "$$\n",
    "R^2 = 1- \\frac{\\sum_{j=1}^n \\left(y^{(j)} - \\hat{y}^{(j)}\\right)^2}{\\sum_{j=1}^n \\left(y^{(j)} - \\bar{y}\\right)^2}\n",
    "$$\n",
    "Le coefficient $R^2$ mesure la part de variance expliquée par le modèle linéaire. \n",
    "\n",
    "On considère souvent qu'un coefficient de prédictivité $R^2>0.9$ est le signe d'une qualité suffisante. Un coefficient $R^2<0.5$ est inacceptable pour une utilisation pratique : c'est le signe que, vraisemblablement, le modèle n'est *pas* linéaire."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
